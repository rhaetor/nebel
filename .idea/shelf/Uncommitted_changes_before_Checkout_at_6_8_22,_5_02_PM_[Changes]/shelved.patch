Index: README.adoc
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>= Nebel Utility for Modular Documentation\n\nNebel is a Python command-line tool that implements certain routine tasks associated with creating and managing _modular documentation_. For example, you can use Nebel to create an instance of an assembly, procedure, concept, or reference file.\n\n* xref:installing-nebel[]\n* xref:setting-up-a-content-repository-to-use-neble[]\n* xref:naming-conventions-for-files-and-directories[]\n* xref:creating-content-with-nebel[]\n* xref:adding-symbolic-links-wth-nebel[]\n* xref:splitting-content[]\n* xref:identifying-orphan-files[]\n* xref:renaming-or-moving-files[]\n* xref:backwards-incompatible-change[]\n* xref:nebel-versioning[]\n* xref:nebel-python-interpreter[]\n\n[id=\"installing-nebel\"]\n== Installing Nebel\n\nThe `nebel` utility is easy to install, as it does not require any special or non-standard Python libraries.\n\nPrerequisites:\n\n* Python 2.7 -- installed by default on Fedora Linux.\n\nHow to install:\n\n. Clone this repository, as follows:\n+\n----\ngit clone git@github.com:fbolton/nebel.git\n----\n\n. Add the `bin/nebel` executable to your `PATH`. For example, in the Bash shell:\n+\n----\nexport PATH=/path/to/your/nebel/bin:$PATH\n----\n+\nIdeally, you should add this export command to your `~/.bashrc` file, to make Nebel permanently available.\n\n[id=\"setting-up-a-content-repository-to-use-neble\"]\n== Setting up a content repository to use Nebel\n\nThe `nebel` utility is designed to manage modular content in a documentation repository (presumed to be a Git repository, although that is not a requirement). Nebel imposes certain requirements on the directory structure of the content repository, as follows:\n\n* There must be a `nebel.cfg` file in the top level of the working directory tree.\nThis file is required by Nebel and `nebel` commands _must_ be issued in the same directory as `nebel.cfg`, otherwise `nebel` returns an error.\n\n* Modules (that is, procedures, concepts, and references) must be stored under a `modules` directory.\nCreate a `modules` directory to store the module files:\n+\n----\nmkdir modules\n----\n\n* Assemblies must be stored under an `assemblies` directory.\nCreate an `assemblies` directory to store the assembly files:\n+\n----\nmkdir assemblies\n----\n\n[id=\"naming-conventions-for-files-and-directories\"]\n== Naming conventions for files and directories\n\n* xref:naming-conventions-for-modules[]\n* xref:naming-conventions-for-assemblies[]\n\n[id=\"naming-conventions-for-modules\"]\n=== Naming convention for modules\n\nFiles under the `modules` directory use the following format for their file names:\n\n[source,subs=\"=macros\"]\n----\nmodules/CATEGORY/TYPE-MODULE_ID_.adoc\n----\n\n`CATEGORY`:: Provides a general way to group related modules in the context of modular documentation. A category replaces the concept of a book or guide. You can use compound categories -- such as `camel/enterprise-integration-patterns` -- which implicitly introduces more subdirectories for the sub-categories.\n\n`TYPE`:: A file prefix that indicates what kind of module is represented by the file. The type is one of the following values:\n+\n* `proc` -- procedure module\n* `con` -- concept module\n* `ref` -- reference module\n\n`MODULE_ID`:: The unique ID for the module. The format is words separated by hyphens. In a module's file name, `MODULE_ID` should be identical to the anchor ID that is defined in the module and attached to the module's title.\n\nFor example, consider the `modules/install/proc-intalling-packages.adoc` file. The `modules` category is `install` and the content of this file would start with: \n\n----\n[id=\"installing-packages\"]\n= Installing packages\n----\n\n[id=\"naming-conventions-for-assemblies\"]\n=== Naming convention for assemblies\n\nFiles under the `assemblies` directory use the following format for their file names:\n\n----\nassemblies/CATEGORY/assembly-MODULE_ID.adoc\n----\n\n`CATEGORY` is used in the same way as the categories for modules.\nIn particular, closely related assemblies and modules should share the same category. For example, consider the `assemblies/install/assembly-how-packages-improve-performance.adoc` file. The `assemblies` category is `install` and the content of this file would start with: \n\n----\n[id=\"how-packages-improve-performance\"]\n= How packages improve performance\n----\n\n[id=\"creating-content-with-nebel\"]\n== Creating content with Nebel\n\nNebel can create content in several ways:\n\n* xref:creating-modules-and-assemblies-from-a-csv-file[]\n* xref:creating-modules-from-an-assembly-file[]\n\n[id=\"creating-modules-and-assemblies-from-a-csv-file\"]\n=== Creating modules and assemblies from a CSV file\n\nThe `nebel` utility enables you to create multiple assemblies and modules from a comma-separated values (CSV) file, which you can obtain by exporting content from a spreadsheet.\nThis capability is designed to support a workflow where you design the high-level structure of a guide in a spreadsheet (for example, in Google sheets) and then generate the corresponding assemblies and modules from the spreadsheet data.\n\nA typical CSV file might have a structure like the following:\n\n----\nCategory,UserStory,Type,ModuleID,Title,VerifiedInVersion,QuickstartID,Comments,Jira\ninstalling-on-apache-karaf,\"As an Evaluator, I want to install Fuse on Karaf, so that I can try it out quickly and discover whether it meets my needs.\",assembly,install-karaf-for-evaluator,,,,Evaluator only has access to the kits published on the developer site. Evaluators like to use an IDE and probably have a Windows machine.,\ninstalling-on-apache-karaf,\"As a Developer, I want to install Fuse on Karaf, so that I can develop Karaf applications on my local machine.\",assembly,install-karaf-for-developer,,,,Developer is probably not that worried about which patch they install. Probably wants to configure Maven properly.,\n----\n\nWhen you use a spreadsheet to plan modules, there must be columns for `Category`, `Type`, `Level` and `ModuleID`.\nHowever, if you are using the spreadsheet to define the high-level structure of a guide, you will almost certainly want to include the `UserStory` column as well.\nSome of the additional columns are preserved as metadata (written into comments in the generated module and assembly files), whilst other additional columns are ignored.\n\nGiven a CSV file, `sample.csv`, you can generate the corresponding modules and assemblies by entering the following command in your content repository:\n\n----\nnebel create-from sample.csv\n----\n\nWhen using a Google sheet to plan assemblies and modules, the `Levels` column enables you to specify a nesting level for each module/assembly by using a positive integer, 1...n. This makes it possible to map out the structure of your book exactly, using arbitrary levels of nesting.\n\nWhen generating content from a sheet (actually, from an exported CSV file), Nebel automatically generates an accompanying `generated_master.adoc` file. This file contains the `include` directives for the top-level items specified in the sheet. This helps you quickly create a skeleton outline of the new book.\n\n[id=\"creating-modules-from-an-assembly-file\"]\n=== Creating modules from an assembly file\n\nThe `nebel` utility can also create modules by scanning an assembly file for AsciiDoc `include::` directives and -- based on the information available in the include directives -- creating corresponding modules that contain template content.\n\nTo create modules from an assembly file:\n\n. Edit an existing assembly file to add some `include::` directives for _some modules that do not exist yet_.\nFor example, in the `assemblies/installing-on-apache-karaf/assembly-install-karaf-for-administrators.adoc`, you could add the following include directives:\n+\n----\n\\include::../../modules/installing-on-apache-karaf/proc-downloading-the-latest-karaf-patch.adoc[leveloffset=+1]\n\n\\include::../../modules/installing-on-apache-karaf/proc-unzipping-karaf-packages.adoc[leveloffset=+1]\n\n\\include::../../modules/installing-on-apache-karaf/proc-creating-karaf-users.adoc[leveloffset=+1]\n----\n\n. In the directory that contains the `nebel.cfg` file, run the following command:  \n+\n----\nnebel create-from assemblies/installing-on-apache-karaf/assembly-install-karaf-for-administrators.adoc\n----\n+\nAfter running this command, you should find three new procedure modules in the `modules/installing-on-apache-karaf` directory.\n\n[id=\"adding-symbolic-links-wth-nebel\"]\n== Adding symbolic links with Nebel\n\nAll content is in the `assemblies` directory and the `modules` directory. For publishing a book, the `master.adoc` file for the book is in another directory, which is a peer to the `assemblies`\ndirectory and `modules` directory. To generate the book, you need a symbolic link from the book directory to each category directory that contains assemblies or modules that contain the content for the book.\n\n[id=\"setting-up-a-book-directory-for-symbolic-links\"]\n=== Setting up a book directory for symbolic links\n\nIn a book directory, before you add symbolic links to category directories, add an `assemblies` directory, an `images` directory, and a `modules` directory.\nFor example, suppose the name of the book directory is `installing-on-jboss-eap`. You want the `installing-on-jboss-eap` directory to contain:\n\n----\nassemblies\nimages\nmodules\nmaster-docinfo.xml\nmaster.adoc\n----\n\n[id=\"running-nebel-to-add-symbolic-links\"]\n=== Running Nebel to add symbolic links\n\nTo run nebel to create symbolic links, the command line has the following form:\n\n----\nnebel book BOOK_DIRECTORY_NAME -c \"CATEGORY1,CATEGORY2,...CATEGORYN\"\n----\n\nReplace `BOOK_DIRECTORY_NAME` with the name of the directory that contains the book for which you are adding symbolic links to category directories.\nIn the quotation marks, insert the name of each category directory for which you want symbolic links.\nFor example, the following command adds symbolic links to the directory that contains the book, Installing on JBoss EAP:\n\n----\nnebel book installing-on-jboss-eap -c \"installing-on-jboss-eap,maven\"\n----\n\nIn the `installing-on-jboss-eap/assemblies` directory, the example command adds symbolic links to:\n\n----\nassemblies/installing-on-jboss-eap\nassemblies/maven\n----\n\nIn the `installing-on-jboss-eap/modules` directory, the example command adds symbolic links to:\n\n----\nmodules/installing-on-jboss-eap\nmodules/maven\n----\n\nIn the `installing-on-jboss-eap/images` directory, the example command adds symbolic links to:\n\n----\nimages/installing-on-jboss-eap\nimages/maven\n----\n\nAt a later time, if you add a new category in the main `assemblies` directory or in the main `modules` directory,\nyou can run the command again and specify only the new category or categories.\n\n[id=\"splitting-content\"]\n== Splitting content in one file into assemblies and modules\n\nYou can split an existing AsciiDoc file into assemblies and modules using Nebel's annotation approach. \nContent that you want to convert must be organized in sections that lend themselves to being assemblies, concept modules, procedure modules, or reference modules. \n\nThere are two main use cases for using `nebel split`:\n\n* You want to convert legacy content to modularized content. In this situation, you iteratively add annotations and run `nebel split` until you get the output that you want. Thereafter, you maintain the assemblies and modules created by running `nebel split`. That is, you update the assembly and module files; you no longer update the legacy content. \n\n* Fewer, larger files are preferred over many, smaller, assembly and module files. Usually, this goes with a preference for shorter headings in place of the longer headings that have the distinguishing qualifications required in modular documentation. Upstream communities often prefer larger files and shorter headings. In this situation, each time you fetch the larger files, for example, to reuse the content downstream, there is a script that runs `nebel split` to create the assemblies and modules. You maintain the larger files; you do not edit the generated assemblies and modules. \n\nThe following topics provide information and instructions for doing this:\n\n* <<annotating-a-file-for-conversion-to-assemblies-and-modules>>\n* <<generating-assemblies-and-modules-from-annotated-files>>\n* <<nebel-split-annotations-reference>>\n* <<nebel-split-command-reference>>\n\n[id=\"annotating-a-file-for-conversion-to-assemblies-and-modules\"]\n=== Annotating a file for conversion to assemblies and modules\n\nTo prepare a non-modularized AsciiDoc file for conversion to assemblies and modules, add required annotations to the file.\n\n.Prerequisites\n\n* You edited the content to be split so that it is easy to distinguish whether a section is an assembly, a concept module, a procedure module or a reference module.  \n\n.Procedure\n\n. In a text editor, open the AsciiDoc file to be split.\n. Find the first, top-level heading in the file.\nFor example, the top-level heading might look like this:\n+\n----\n[[debezium-mysql-connector]]\n= Debezium Connector for MySQL\n----\n\n. Decide which _category_ this content should be in. \n\n. Determine the content type, that is, whether this section should be an assembly, a concept module, a procedure module, or a reference module. As this is a top-level heading, it is likely that you would want to convert the initial content into an assembly.\n\n. Insert comments that specify the category and the type. \n+\nFor example, suppose that the section should be an assembly in the `debezium-using` category. Add the following comments (annotations) to the file immediately before the heading:\n+\n----\n// Category: debezium-using\n// Type: assembly\n[[mysql-connector]]\n= Debezium Connector for MySQL\n----\n+\nThe default behavior is that the category you specify for the top-level heading \napplies to all subheadings.\n+\nIf you do not specify the category, it defaults to the name of the directory containing the file being split or to a directory named `default`.\n\n. Move down to the next section heading, which is most likely a subheading \nof the preceding heading. In other words, the next section heading is probably \nindicated by two equals signs, `==`. \n\n. To convert this subheading and its content to a module, decide the content's type.\n\n. Add a comment that specifies the content's type as \n`concept`, `procedure`, or `reference`. For example:\n+\n----\n// Type: concept\n[[overview]]\n== Overview of MySQL Connector\n----\n+\nIt is _not_ necessary to specify a `Category` annotation for\nsubheadings. \n+\nAny subheadings in this module section do not require annotations. \nWhen you run Nebel to split the content, the utility \nmaps any subheadings here to subheadings in the enclosing module. \nThis is the default behavior. \n\n. _Optional._ To override the section's anchor ID, which appears in \ndouble square brackets, `[[  ]]`, insert a `ModuleID` annotation before the heading.\nFor example:\n+\n----\n// Type: concept\n// ModuleID: overview-of-debezium-mysql-connector\n[[overview]]\n== Overview of MySQL Connector\n----\n+\nThe generated module or assembly will have the anchor ID that you specify\nin the `ModuleID` annotation. \n\n. _Optional_. To override the section's title, insert a `Title` annotation before the heading.\nFor example:\n+\n----\n// Type: concept\n// ModuleID: overview-of-debezium-mysql-connector\n// Title: Overview of Debezium MySQL connector\n[[overview]]\n== Overview of MySQL Connector\n----\n+\nThe generated module or assembly will have the title that you specify\nin the `Title` annotation. \n\n. Proceed through the file, adding `Category`, `Type`, `ModuleID`, and `Title` annotations before subsection headings as needed.\n\n. Save and close the file. \n\n.Examples\n\n* link:https://raw.githubusercontent.com/debezium/debezium/master/documentation/modules/ROOT/pages/connectors/mysql.adoc[Debezium connector for MySQL]\n\n* link:https://raw.githubusercontent.com/debezium/debezium/master/documentation/modules/ROOT/pages/connectors/postgresql.adoc[Debezium connector for PostgreSQL]\n\n.Next steps\n\nTo generate assemblies and modules for a book, repeat this procedure for each of\nthe book's files. \n\n[id=\"generating-assemblies-and-modules-from-annotated-files\"]\n=== Generating assemblies and modules from annotated files\n\nWhen an AsciiDoc file has modular documentation annotations, \nyou can run Nebel to convert it to assemblies and modules. \n\n.Prerequisites\n\n* You added `Type` annotations, and optionally `Category`, `Title` and `ModuleID` annotations, to one or more AsciiDoc files. \n\n* link:https://github.com/fbolton/nebel[The Nebel utility is installed.]\n\n* You have the latest Nebel updates. If you have not run Nebel in a while, change to your local `nebel` directory and run `git pull`.\n\n* The documentation directory in which you want to run `nebel` meets these conditions:\n** The top-level directory contains a `nebel.cfg` file.\nFor example, the `fuse7/docs` directory contains a `nebel.cfg` file. \nYou can copy `nebel.cfg` to a directory if you need to. \n** The directory is \n<<how-modularized-integration-documentation-repositories-are-organized,organized for modularization.>>\n\n.Procedure\n\n. Open a new shell prompt. \n. Navigate to the directory that contains the `nebel.cfg` file.\n. To generate assemblies and modules from one AsciiDoc file, run `nebel` using the following command line format: \n+\n`nebel split --legacybasedir LEGACYBASEDIR ANNOTATED_FILE.adoc`\n+\n`LEGACYBASEDIR` specifies the root directory of the file being split, such that the immediate subdirectories of `LEGACYBASEDIR` implicitly define the default categories.\n+\n`ANNOTATED_FILE` is the path to the annotated AsciiDoc file.\n+\nFor example, if the annotated file is located at `./connectors/mysql.adoc`, you could run a command like this: \n+\n`nebel split --legacybasedir . connectors/mysql.adoc`\n+\nThis would store the generated assemblies in the `assemblies/connectors` directory and the generated modules in the `modules/connectors` directory.\nThe generated category defaults to `connectors`, because `connectors` is the immediate subdirectory of the specified `LEGACYBASEDIR` (`.` directory).\n+\nAlternatively, you can specify only the name of the annotated file, for example: \n+\n`nebel split upstream/debezium/debezium-1.2/documentation/modules/ROOT/pages/connectors/mysql.adoc`\n+\nTo generate assemblies and modules from multiple files, use the wildcard, which is a pair of curly braces, `{}`.\nFor example:\n+\n`nebel split --legacybasedir . connectors/{}.adoc`\n\n. _Optional_. After splitting files, you can run Nebel to fix cross-references that changed because of `ModuleID` annotations. For example: \n+\n`nebel update --fix-links -a upstream/debezium/attributes.adoc,attributes.adoc,attributes-links.adoc -c debezium-using`\n+\n`-a`:: Specifies a comma-separated list of attribute files that that Nebel needs to update references. You must specify the path for the repository's `attributes.adoc` and `attributes-links.adoc` files. If the directory uses any other attributes files, you must specify them as well. This sample command line specifies the `attributes.adoc` file in the `upstream/debezium` directory. \n \n`-c`:: Specifies the scope of the content in which Nebel updates links. Specify one or more, comma-separated category names. In this example, Nebel fixes links that are in the `debezium-using` category.  \n\n[id=\"nebel-split-annotations-reference\"]\n=== Nebel split annotations reference\n\nIn a non-modular AsciiDoc file, a Nebel annotation is a one-line \ncomment immediately before a section heading. There cannot be blank lines between the annotation and the heading it applies to. An annotation comment has the following\nformat:\n\n`// ANNOTATION_NAME: ANNOTATION_VALUE`\n\nEach annotation that Nebel can interpret \nand process when splitting large, current files or legacy files into assemblies and modules is described here. \n\n* <<category>>\n* <<type>>\n* <<topictype>>\n* <<moduleid>>\n* <<title>>\n\n[id=\"category\"]\n==== `Category`\n\nThis annotation specifies the category for the content in the following section. The category is the \nsubdirectory of the `assemblies` directory or the `modules` directory in which \nNebel will store the generated file. After you specify a category for a \nparticular section, it applies to _all_ of its subsections, and, hence, to \nall of the assemblies and modules generated from those subsections.\n\nIt is typically necessary to set the `Category` annotation on only the  top-level section heading in a file.\nThe rest of the subsections in the file are then implicitly mapped to the same category.\nIf you do not specify the `Category` annotation, it defaults to one of the following: \n\n* The directory that contains the AsciiDoc file being split, as indicated by the `--legacybasedir` option in the `nebel split` command\n* The `assemblies/default` and `modules/default` directories, which `nebel split` creates if `--legacybasedir` was not specified in the `nebel split` command\n\nFor example, given the following `Category` annotation:\n----\n// Category: integration-nebel\n// Type: assembly\n[id=\"splitting-content\"]\n== Splitting content \n----\n\nNebel creates the `assembly-splitting-content.adoc` file in the `assemblies/integration-nebel/` directory.\n\n[id=\"type\"]\n==== `Type`\nThis annotation Specifies the kind of module to map the following section to. \n\n.Descriptions of `Type` annotation values\n[cols='1,4',options=\"header\"]\n|===\n|Value | Description\n\n|`assembly`\n|Maps the section to an assembly.\n\n|`concept`\n|Maps the section to a concept module.\n\n|`procedure`\n|Maps the section to a procedure module.\n\n|`reference`\n|Maps the section to a reference module.\n\n|`continue`\n|Absorbs the section into the preceding module or assembly. \nIt becomes a subsection of the preceding module or assembly. \n\n|`skip`\n|Skips the section. It does not appear in the generated assembly or module.\n|===\n\nIf you do not specify a `Type` annotation for a section, the section \nbecomes a subsection of the nearest enclosing module or assembly.\n\n[id=\"topictype\"]\n==== `TopicType`\nThis annotation is an alias of `Type`.\nIn other words, you can use either the `TopicType` annotation alias or the `Type` annotation to specify the module or assembly type.\n\n[id=\"moduleid\"]\n==== `ModuleID`\n\nThis annotation specifies a new value for the section's anchor ID. \nIn the generated module, the `ModuleID` value\nthat you specify in this annotation replaces the ID in the unsplit source file.  \n\nIf you want the generated section to have a different ID \nfrom the content being split, use this annotation instead of changing the existing ID directly. This \nmakes it possible for Nebel to update cross-references to this section so that they specify the new anchor ID. \nFor example:\n\n----\n// Category: integration-nebel\n// Type: assembly\n// ModuleID: splitting-content-by-inserting-annotations\n[id=\"splitting-content\"]\n== Splitting content \n----\n\nWith these annotations, Nebel creates the `assembly-splitting-content-by-inserting-annotations.adoc` file and the anchor ID in the file is \n\n`[id=\"splitting-content-by-inserting-annotations\"]`\n\n[id=\"title\"]\n==== `Title`\nThis annotation specifies a new title (heading) for the section.\nIn the generated module, Nebel replaces the heading that is in the file being split with the `Title` value that you specify in this  annotation.\n\nThis is useful for keeping short headings upstream and also having longer, descriptive headings downstream.\nFor example:\n\n----\n// Type: concept\n// ModuleID: overview-of-splitting-asciidoc-by-using-annotations\n// Title: Overview of splitting AsciiDoc by using annotations\n[[Overview]]\n== Overview\n----\n\nWith these annotations, Nebel creates the `con-overview-of-splitting-asciidoc-by-using-annotations.adoc` file and the heading in the file is \n\n`== Overview of splitting AsciiDoc by using annotations`\n\n[id=\"nebel-split-command-reference\"]\n=== Nebel split command reference\n\nThe `nebel split` command, has the following format:  \n\n[source,options=\"+macros\"]\n----\nnebel split [-h] [options] FROM_FILE\n----\n\nRun `nebel split -h` to display help for the options. Replace `FROM_FILE` with the name of the `.adoc` file to split. \n\n.Descriptions of `nebel split` command options\n[cols='1,3',options=\"header\"]\n|===\n|Option \n|Description\n\n|`-a`\n|One or more attribute files (comma separated) that Nebel needs to resolve links to included files in the assemblies that Nebel generates. Attributes are often in the path names specified in `include` statements. When Nebel splits content, it must be able to resolve (find) included files. \n\n|`--conditions`\n|One or more condition names (comma separated) specified in `ifdef` or `ifndef` statements in content to be split. Nebel keeps or deletes content tagged with the conditions you specify depending on whether the source specifies `ifdef` or `ifndef`. Nebel does this before it splits the content. +\n +\nA common scenario is that you want to delete upstream-only content when you are splitting content into files that will be used downstream. For example, suppose that you tag upstream-only content with `ifdef::community[]` and you tag downstream-only content with `ifdef::product[]`. When you run `nebel split` and specify `--conditions product`, Nebel keeps content tagged with `product` and deletes content tagged with `community`. +\n +\nNow suppose that you specify `ifndef::community[]` statements to tag content that is downstream-only. To keep content tagged by `ifndef::community[]` in the generated assemblies and modules, run `nebel split` with `--conditions community`. \n +\nIf the content being split contains `ifdef` or `ifndef` statements and you run `nebel split` without specifying the `--conditions` option, Nebel keeps and splits all content. This is likely to lead to erroneous output. \n\n|`--category-prefix`\n|Adds the specified prefix to any generated categories, which can be helpful to distinguish categories from one another. For example, suppose that an annotation instructs Nebel to generate some modules in the category `getting-started`. By running `nebel split` and specifying `--category-prefix debezium`, Nebel would add modules to the `debezium-getting-started` category. +\n +\nIn a split annotation, if you specify, for example, `// Category: debezium-getting-started` and you run `nebel split` with the `--category debezium` option, Nebel adds files to the `debezium-getting-started` category and not to the `debezium-debezium-getting-started` category. \n\n|`--legacybasedir`\n|Specifies the root directory of the file(s) being split. Nebel might use this directory to determine the category for the generated files.\n\n|`--timestamp`\n|Inserts a timestamp in each generated assembly and module file. \n\n|===\n\nHere is an example of a `nebel split` command: \n\n[source,options=\"nowrap\"]\n----\nnebel split -a attributes.adoc --conditions product --category-prefix debezium --legacybasedir upstream/debezium/debezium-$branch/documentation/modules/ROOT/pages upstream/debezium/debezium-$branch/documentation/modules/ROOT/pages/connectors/postgresql.adoc\n----\n\nThis command: \n\n* Is executed in the `integration/docs` directory.\n* Specifies the `attributes.adoc` file that is in the `docs` directory.\n* Instructs `nebel split` to delete content that is tagged with a condition other than `product`. For example, content enclosed in `ifdef::community[]` and `endif::community[]` statements is deleted. \n* Specifies that the names of categories that will contain the generated files begin with `debezium`. \n* Provides the path of the root directory for the file to be split. \n* Specifies the path of the file to be split. \n\n[id=\"identifying-orphan-files\"]\n== Identifying orphan files\n\nYou can run Nebel to identify files that are not included in a `master.adoc` file or an assembly. It is then up to you to determine whether to delete an orphan file or add an `include` statement for an orphan file. \n\nThis is helpful when you regularly run `nebel split` upon fetching updated upstream content. If you change a Nebel annotation in an upstream file, the next time you fetch the file, `nebel split` might create a file with a different name but with the same content as an existing file. For example, suppose an upstream file has this annotation: \n\n----\n// Type: concept\n// ModuleID: descriptions-of-events\n----\n\nYou fetch the upstream file and `nebel split` generates the `con-descriptions-of-events.adoc` file, which is included in an assembly. Later, you change the annotation:\n\n----\n// ModuleID: descriptions-of-debezium-events\n----\n\nThe next time that you fetch the upstream file, `nebel split` generates the `con-descriptions-of-debezium-events.adoc` file and includes this file in an assembly. The `con-descriptions-of-events.adoc` file is no longer included in an assembly; it is an orphan file that you can delete. \n\nThe format for running `nebel orphan` is: \n\n----\nnebel orphan [-h] [-c CATEGORY_LIST] [-a ATTRIBUTE_FILES]\n----\n\n`-h`:: Displays a help message.\n\n`-c CATEGORY_LIST`:: Replace `CATEGORY_LIST` with a comma-separated list of categories. Nebel checks for orphan files by evaluating `include` statements in all `master.adoc` files and in `assemblies` and `modules` subdirectories for only the categories that you specify. If you do not specify the `-c` option, Nebel checks all categories for orphans. \n\n`-a ATTRIBUTE_FILES`:: Replace `ATTRIBUTE_FILES` with a comma-separated list of attribute files that Nebel needs to resolve paths in `include` statements in the categories that the command is checking. \n\nFor example: \n\n----\nnebel orphan -c debezium-using -a attributes.adoc,upstream/debezium/attributes.adoc\n----\n\nThis command resolves `include` statements in assemblies that are in the `debezium-using` category. To do this, Nebel needs the toplevel `attributes.adoc` file, and it also needed the `upstream/debezium/attributes.adoc` file. \n\n[id=\"renaming-or-moving-files\"]\n== Renaming or moving files\n\nThe `nebel mv` command lets you move or rename a file (or files) without breaking any include directives. In particular, this subcommand was originally implemented to assist with renaming modular file prefixes. For example, consider a collection of procedure modules whose file names start with `p_`. To change that prefix to `proc-` you can rename the files by running a command like this:\n\n----\nnebel mv modules/getting-started/p_{}.adoc modules/getting-started/proc-{}.adoc\n----\n\nThe `nebel` utility updates `include` directives as well as links that contain the file names that are being changed.\n\n[id=\"backwards-incompatible-change\"]\n== Backwards-incompatible change to modular file prefixes\n\nPrior to Nebel version 2, Nebel assumed that the underscore, `_`, was the separator for modular file prefixes. For example, file names had prefixes such `proc_`, `con_`, and `ref_`. It was possible to customize the prefixes, by setting some properties in the `nebel.cfg` file, but it was not possible to change the separator to be anything other than an underscore.\n\nStarting with Nebel version 2, however, it is possible to customize file prefixes, including the separator character, by editing settings in the `nebel.cfg` file. For example:\n\n----\n[Nebel]\ndir.assemblies = assemblies\ndir.modules = modules\nprefix.assembly = assembly-\nprefix.procedure = proc-\nprefix.concept = con-\nprefix.reference = ref-\n----\n\n[id=\"nebel-versioning\"]\n== Nebel versioning\n\nNebel now supports a version flag, which you can use to check the particular version you are using, for example:\n\n----\nnebel -v\nNebel 2.1.x (dev release)\n----\n\nHere is a recent version history:\n\n* 1.0.0 -- First numbered version (from April 4, 2020), uses the old convention for modular file prefixes (underscore separator is hardcoded).\n* 2.0.x -- Backwards-incompatible update, uses the new convention for modular file prefixes (separator character is part of the customizable prefix, thus enabling you to use a hyphen separator).\n* 2.1.x -- Supports the new `nebel split` subcommand.\n\n[id=\"nebel-python-interpreter\"]\n== Nebel Python interpreter\n\nThe `nebel` utility is coded for the Python 2 interpreter and does not work with Python 3. On May 15, I modified the `nebel` binary, so that it calls the Python 2 interpreter explicitly (instead of calling the ambiguous Python executable). This ensures that `nebel` also runs correctly on recent Fedora and RHEL releases. In the long run, `nebel` will need to be updated for Python 3.\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/README.adoc b/README.adoc
--- a/README.adoc	(revision 1bc0b02d66cdf9081a5604adef47921f6e7eb456)
+++ b/README.adoc	(date 1654626388510)
@@ -651,12 +651,16 @@
 
 The `nebel` utility updates `include` directives as well as links that contain the file names that are being changed.
 
-[id="backwards-incompatible-change"]
-== Backwards-incompatible change to modular file prefixes
+[id="modular-file-prefixes"]
+== Modular file prefixes
+
+The default modular file prefix is the underscore, `_`.
 
-Prior to Nebel version 2, Nebel assumed that the underscore, `_`, was the separator for modular file prefixes. For example, file names had prefixes such `proc_`, `con_`, and `ref_`. It was possible to customize the prefixes, by setting some properties in the `nebel.cfg` file, but it was not possible to change the separator to be anything other than an underscore.
+was the separator for modular file prefixes. For example, file names had prefixes such `proc_`, `con_`, and `ref_`. It was possible to customize the prefixes, by setting some properties in the `nebel.cfg` file, but it was not possible to change the separator to be anything other than an underscore.
 
-Starting with Nebel version 2, however, it is possible to customize file prefixes, including the separator character, by editing settings in the `nebel.cfg` file. For example:
+You can customize the file prefixes, including the separator character, by editing settings in the `nebel.cfg` file.
+
+For example:
 
 ----
 [Nebel]
@@ -669,22 +673,18 @@
 ----
 
 [id="nebel-versioning"]
-== Nebel versioning
+== Check the Nebel version
 
-Nebel now supports a version flag, which you can use to check the particular version you are using, for example:
+You can use `-v` to check which version you are using.
+
+For example:
 
 ----
 nebel -v
 Nebel 2.1.x (dev release)
 ----
 
-Here is a recent version history:
-
-* 1.0.0 -- First numbered version (from April 4, 2020), uses the old convention for modular file prefixes (underscore separator is hardcoded).
-* 2.0.x -- Backwards-incompatible update, uses the new convention for modular file prefixes (separator character is part of the customizable prefix, thus enabling you to use a hyphen separator).
-* 2.1.x -- Supports the new `nebel split` subcommand.
-
 [id="nebel-python-interpreter"]
 == Nebel Python interpreter
 
-The `nebel` utility is coded for the Python 2 interpreter and does not work with Python 3. On May 15, I modified the `nebel` binary, so that it calls the Python 2 interpreter explicitly (instead of calling the ambiguous Python executable). This ensures that `nebel` also runs correctly on recent Fedora and RHEL releases. In the long run, `nebel` will need to be updated for Python 3.
+The `nebel` utility only works Python 3.
\ No newline at end of file
Index: nebel/commands.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>'''\nCreated on January 2, 2019\n\n@author fbolton\n'''\n\nimport os\nimport re\nimport sys\nimport tempfile\nimport shutil\nimport argparse\nimport nebel.context\nimport nebel.factory\nimport datetime\nimport glob\nimport hashlib\nimport subprocess\n\nclass Tasks:\n    def __init__(self, context):\n        self.context = context\n\n    def _create(self, args, metadata):\n        metadata['Category'] = args.CATEGORY\n        metadata['ModuleID'] = args.MODULE_ID\n        if args.user_story:\n            metadata['UserStory'] = args.user_story\n        if args.title:\n            metadata['Title'] = args.title\n        if args.jira:\n            metadata['Jira'] = args.jira\n        if args.parent_assemblies:\n            metadata['ParentAssemblies'] = args.parent_assemblies\n        modulefile = self.context.moduleFactory.create(metadata)\n        if args.parent_assemblies:\n            for assemblyfile in args.parent_assemblies.split():\n                self.add_include_to_assembly(assemblyfile, modulefile)\n\n    def create_assembly(self,args):\n        metadata = {'Type':'assembly'}\n        self._create(args, metadata)\n\n    def create_procedure(self,args):\n        metadata = {'Type':'procedure'}\n        self._create(args, metadata)\n\n    def create_concept(self,args):\n        metadata = {'Type':'concept'}\n        self._create(args, metadata)\n\n    def create_reference(self,args):\n        metadata = {'Type':'reference'}\n        self._create(args, metadata)\n\n    def add_include_to_assembly(self, assemblyfile, includedfile, leveloffset=1):\n        if not os.path.exists(assemblyfile):\n            print 'WARN: Referenced assembly file does not exist:' + assemblyfile\n            return\n        # Create temp file\n        fh, abs_path = tempfile.mkstemp()\n        with os.fdopen(fh, 'w') as new_file:\n            with open(assemblyfile) as old_file:\n                # Find the position in the file to add the include directive\n                position_of_new_include = -1\n                len_old_file = 0\n                for k, line in enumerate(old_file):\n                    len_old_file += 1\n                    if line.lstrip().startswith('include::'):\n                        position_of_new_include = k\n                    if line.lstrip().startswith('//INCLUDES'):\n                        position_of_new_include = k\n                if position_of_new_include == -1:\n                    # Default to end of the file\n                    position_of_new_include = len_old_file - 1\n                # Reset the file stream to the beginning\n                old_file.seek(0)\n                # Write the new file, with added include\n                for k, line in enumerate(old_file):\n                    new_file.write(line)\n                    if k == position_of_new_include:\n                        relpath = os.path.relpath(includedfile, os.path.dirname(assemblyfile))\n                        new_file.write('\\n')\n                        new_file.write('include::' + relpath + '[leveloffset=+' + str(leveloffset) + ']\\n\\n')\n        # Remove original file\n        os.remove(assemblyfile)\n        # Move new file\n        shutil.move(abs_path, assemblyfile)\n\n\n    def create_from(self,args):\n        fromfile = args.FROM_FILE\n        if fromfile.endswith('.csv'):\n            self._create_from_csv(args)\n            return\n        elif fromfile.startswith(self.context.ASSEMBLIES_DIR)\\\n                and fromfile.endswith('.adoc')\\\n                and os.path.basename(fromfile).startswith(self.context.ASSEMBLY_PREFIX):\n            self._create_from_assembly(args)\n            return\n        else:\n            print 'ERROR: Unknown file type [' + fromfile + ']: must end either in .csv or .adoc'\n            sys.exit()\n\n    def type_of_file(self, basename):\n        # ToDo: Should be more flexible at recognizing file types\n        if basename.startswith(self.context.ASSEMBLY_PREFIX):\n            return 'assembly'\n        elif basename.startswith(self.context.PROCEDURE_PREFIX):\n            return 'procedure'\n        elif basename.startswith(self.context.CONCEPT_PREFIX):\n            return 'concept'\n        elif basename.startswith(self.context.REFERENCE_PREFIX):\n            return 'reference'\n        else:\n            return 'module'\n\n    def moduleid_of_file(self, basename):\n        base, ext = os.path.splitext(basename)\n        if base.startswith(self.context.ASSEMBLY_PREFIX):\n            return self.context.moduleFactory.lreplace(self.context.ASSEMBLY_PREFIX, '', base)\n        elif base.startswith(self.context.PROCEDURE_PREFIX):\n            return self.context.moduleFactory.lreplace(self.context.PROCEDURE_PREFIX, '', base)\n        elif base.startswith(self.context.CONCEPT_PREFIX):\n            return self.context.moduleFactory.lreplace(self.context.CONCEPT_PREFIX, '', base)\n        elif base.startswith(self.context.REFERENCE_PREFIX):\n            return self.context.moduleFactory.lreplace(self.context.REFERENCE_PREFIX, '', base)\n        else:\n            return base\n\n    def title_to_id(self, title):\n        title = title.strip()\n        # Remove any character which is not a dash, underscore, alphanumeric, or whitespace\n        title = re.sub(r'[^0-9a-zA-Z_\\-\\s]+', '', title)\n        # Replace one or more contiguous whitespaces with a dash\n        title = re.sub(r'\\s+', '-', title)\n        return title\n\n    def _create_from_assembly(self,args):\n        asfile = args.FROM_FILE\n        regexp = re.compile(r'^\\s*include::[\\./]*modules/([^\\[]+)\\[[^\\]]*\\]')\n        with open(asfile, 'r') as f:\n            for line in f:\n                result = regexp.search(line)\n                if result is not None:\n                    modulefile = result.group(1)\n                    category, basename = os.path.split(modulefile)\n                    type = self.type_of_file(basename)\n                    if type is not None and basename.endswith('.adoc'):\n                        print modulefile\n                        metadata = {}\n                        metadata['Type'] = type\n                        metadata['Category'] = category\n                        metadata['ModuleID'] = self.moduleid_of_file(basename)\n                        metadata['ParentAssemblies'] = asfile\n                        self.context.moduleFactory.create(metadata)\n\n\n    def adoc_split(self, args):\n        frompattern = os.path.normpath(args.FROM_FILE)\n        fromfiles = glob.glob(frompattern.replace('{}', '*'))\n        if args.attribute_files:\n            attrfilelist = args.attribute_files.strip().split(',')\n            self.context.parse_attribute_files(attrfilelist)\n        else:\n            attrfilelist = None\n        if args.conditions:\n            selectedconditions = args.conditions.strip().split(',')\n        else:\n            selectedconditions = None\n        for fromfile in fromfiles:\n            metadata = {}\n            categoryname = 'default'\n            if args.legacybasedir:\n                if not os.path.exists(args.legacybasedir):\n                    print 'ERROR: No such base directory: ' + args.legacybasedir\n                    sys.exit()\n                relativedir = os.path.dirname(os.path.relpath(fromfile, args.legacybasedir))\n                categoryname = relativedir.replace(os.path.sep, '-')\n            if args.category_prefix:\n                categoryname = args.category_prefix + '-' + categoryname\n            metadata['Category'] = categoryname\n            equalssigncount = 0\n            lines = self._resolve_includes(fromfile)\n            indexofnextline = 0\n            self._parse_from_annotated(metadata, fromfile, lines, indexofnextline, equalssigncount, selectedconditions, args.timestamp)\n\n    def _parse_from_annotated(\n            self,\n            metadata,\n            fromfilepath,\n            lines,\n            indexofnextline,\n            equalssigncount,\n            selectedconditions = None,\n            timestamp = False,\n            showcontentstack = [],\n            currconditionstack = []\n    ):\n        # Define some enums for state machine\n        REGULAR_LINES = 0\n        TENTATIVE_PARSING = 1\n\n        # Define action enums\n        NO_ACTION = 0\n        CREATE_SUBSECTION = 1\n        CREATE_MODULE_OR_ASSEMBLY = 2\n        END_CURRENT_MODULE = 3\n\n        # Initialize Boolean state variables\n        parsing_state = REGULAR_LINES\n        expecting_title_line = False\n        module_complete = False\n        if (selectedconditions is not None) and (len(selectedconditions) > 0):\n            isconditionalizeactive = True\n            showcontent = True\n        else:\n            isconditionalizeactive = False\n            showcontent = True\n\n        # Define regular expressions\n        regexp_metadata = re.compile(r'^\\s*//\\s*(\\w+)\\s*:\\s*(.*)')\n        regexp_id_line1 = re.compile(r'^\\s*\\[\\[\\s*(\\S+)\\s*\\]\\]\\s*$')\n        regexp_id_line2 = re.compile(r'^\\s*\\[id\\s*=\\s*[\\'\"]\\s*(\\S+)\\s*[\\'\"]\\]\\s*$')\n        regexp_ifdef    = re.compile(r'^ifdef::([^\\[]+)\\[\\]')\n        regexp_ifdef_single = re.compile(r'^ifdef::([^\\[]+)\\[([^\\]]+)\\]')\n        regexp_ifndef   = re.compile(r'^ifndef::([^\\[]+)\\[\\]')\n        regexp_ifndef_single = re.compile(r'^ifndef::([^\\[]+)\\[([^\\]]+)\\]')\n        regexp_ifeval   = re.compile(r'^ifeval::\\[([^\\]]*)\\]')\n        regexp_endif    = re.compile(r'^endif::([^\\[]*)\\[\\]')\n        regexp_title = re.compile(r'^(=+)\\s+(\\S.*)')\n\n        childmetadata = {}\n        parsedcontentlines = []\n\n        while not module_complete:\n            # Check for end of file\n            if indexofnextline >= len(lines):\n                if ('Type' in metadata) and (metadata['Type'].lower() == 'skip'):\n                    # Don't save current content\n                    return ('', len(lines))\n                elif 'Type' in metadata:\n                    generated_file = self.context.moduleFactory.create(metadata, parsedcontentlines, clobber=True)\n                    return (generated_file, len(lines))\n                else:\n                    return ('', len(lines))\n\n            if isconditionalizeactive:\n                line = lines[indexofnextline]\n                result = regexp_ifdef_single.search(line)\n                if result is not None:\n                    conditionname = result.group(1)\n                    conditionline = result.group(2)\n                    if conditionname in selectedconditions:\n                        # Replace current line with conditional text\n                        lines[indexofnextline] = conditionline\n                    else:\n                        # Skip to next line\n                        indexofnextline += 1\n                    continue\n                result = regexp_ifndef_single.search(line)\n                if result is not None:\n                    conditionname = result.group(1)\n                    conditionline = result.group(2)\n                    if conditionname not in selectedconditions:\n                        # Replace current line with conditional text\n                        lines[indexofnextline] = conditionline\n                    else:\n                        # Skip to next line\n                        indexofnextline += 1\n                    continue\n                result = regexp_ifdef.search(line)\n                if result is not None:\n                    conditionname = result.group(1)\n                    currconditionstack.append(conditionname)\n                    showcontentstack.append(showcontent)\n                    if conditionname not in selectedconditions:\n                        showcontent = False\n                    # Do not include tagged line in output\n                    indexofnextline += 1\n                    continue\n                result = regexp_ifndef.search(line)\n                if result is not None:\n                    conditionname = result.group(1)\n                    currconditionstack.append(conditionname)\n                    showcontentstack.append(showcontent)\n                    if conditionname in selectedconditions:\n                        showcontent = False\n                    # Do not include tagged line in output\n                    indexofnextline += 1\n                    continue\n                result = regexp_ifeval.search(line)\n                if result is not None:\n                    currconditionstack.append('')\n                    showcontentstack.append(showcontent)\n                    print ('WARNING: ifeval not supported: defaults to showing content')\n                    # Do not include tagged line in output\n                    indexofnextline += 1\n                    continue\n                result = regexp_endif.search(line)\n                if result is not None:\n                    conditionname = result.group(1)\n                    matchcondition = currconditionstack.pop()\n                    showcontent = showcontentstack.pop()\n                    if (conditionname) and (conditionname != matchcondition):\n                        print ('WARNING: Unmatched condition tags: ' + conditionname + '!=' + matchcondition)\n                    # Do not include tagged line in output\n                    indexofnextline += 1\n                    continue\n            if not showcontent:\n                # Content is currently tagged off - skip this line\n                indexofnextline += 1\n                continue\n\n            if parsing_state == REGULAR_LINES:\n                line = lines[indexofnextline]\n                if (regexp_metadata.search(line) is None) and (regexp_id_line1.search(line) is None) and (regexp_id_line2.search(line) is None) and (regexp_title.search(line) is None):\n                    # Regular line\n                    parsedcontentlines.append(line)\n                    indexofnextline += 1\n                else:\n                    # Switch state\n                    parsing_state = TENTATIVE_PARSING\n                    expecting_title_line = False\n                    index_of_tentative_block = indexofnextline\n                    tentativecontentlines = []\n            elif parsing_state == TENTATIVE_PARSING:\n                line = lines[indexofnextline]\n                tentativecontentlines.append(line)\n                indexofnextline += 1\n                # Skip blank lines\n                if line.strip() == '':\n                    continue\n                # Parse title line\n                result = regexp_title.search(line)\n                if result is not None:\n                    childequalssigncount = len(result.group(1))\n                    title = result.group(2)\n                    if 'Title' not in childmetadata:\n                        childmetadata['Title'] = title\n                    else:\n                        childmetadata['ConvertedFromTitle'] = title\n                    if ('Type' in metadata) and (metadata['Type'].lower() == 'skip'):\n                        childmetadata['Type'] = 'skip'\n                    action = NO_ACTION\n                    # Decide how to proceed based on level of new heading\n                    if childequalssigncount > equalssigncount:\n                        if 'Type' not in childmetadata:\n                            action = CREATE_SUBSECTION\n                        else:\n                            action = CREATE_MODULE_OR_ASSEMBLY\n                    elif childequalssigncount == equalssigncount:\n                        if ('Type' in childmetadata) and (childmetadata['Type'].lower() == 'continue'):\n                            action = CREATE_SUBSECTION\n                        else:\n                            action = END_CURRENT_MODULE\n                    else:\n                        # childequalssigncount < equalssigncount\n                        action = END_CURRENT_MODULE\n                    # Perform action\n                    if action == CREATE_SUBSECTION:\n                        # It's a simple subsection, not a module or assembly\n                        # Reformat heading as a simple heading (starts with .)\n                        lastline = tentativecontentlines.pop()\n                        lastline = '.' + lastline.replace('=', '').lstrip() + '\\n'\n                        # Put back tentative lines\n                        for tentativeline in tentativecontentlines:\n                            parsedcontentlines.append(tentativeline)\n                        parsedcontentlines.append(lastline)\n                    elif action == CREATE_MODULE_OR_ASSEMBLY:\n                        if ('ModuleID' not in childmetadata):\n                            print 'ERROR: Heading ' + title + ' must have a module ID.'\n                            sys.exit()\n                        if ('Category' not in childmetadata):\n                            childmetadata['Category'] = metadata['Category']\n                        childmetadata['ConversionStatus'] = 'raw'\n                        if timestamp: childmetadata['ConversionDate'] = str(datetime.datetime.now())\n                        childmetadata['ConvertedFromFile'] = fromfilepath\n                        (generated_file, indexofnextline) = self._parse_from_annotated(\n                            childmetadata,\n                            fromfilepath,\n                            lines,\n                            indexofnextline,\n                            childequalssigncount,\n                            selectedconditions,\n                            timestamp,\n                            showcontentstack,\n                            currconditionstack\n                        )\n                        #if ('Type' in childmetadata) and (childmetadata['Type'].lower() == 'assembly'):\n                        #    print ('include::' + generated_file + '[leveloffset=+1]')\n                        childmetadata = {}\n                        parsedcontentlines.append('\\n')\n                        if generated_file:\n                            parsedcontentlines.append('include::../../' + generated_file + '[leveloffset=+1]\\n\\n')\n                    elif action == END_CURRENT_MODULE:\n                        if metadata['Type'].lower() == 'skip':\n                            # Don't save current content and back up to the start of the tentative block\n                            return ('', index_of_tentative_block)\n                        # Save the current content\n                        generated_file = self.context.moduleFactory.create(metadata, parsedcontentlines, clobber=True)\n                        return (generated_file, index_of_tentative_block)\n                    # Switch state\n                    parsing_state = REGULAR_LINES\n                    expecting_title_line = False\n                    childmetadata = {}\n                    continue\n                # Parse metadata line\n                result = regexp_metadata.search(line)\n                if (result is not None) and not expecting_title_line:\n                    metadata_name = result.group(1)\n                    metadata_value = result.group(2)\n                    # Make 'TopicType' an alias for 'Type' (preferred upstream)\n                    if metadata_name == 'TopicType':\n                        metadata_name = 'Type'\n                    if metadata_name in self.context.allMetadataFields:\n                        childmetadata[metadata_name] = metadata_value\n                    else:\n                        print 'WARNING: Unknown metadata \"' + metadata_name + '\" in file ' + fromfilepath\n                    #print 'Metadata: ' + metadata_name + ' = ' + metadata_value\n                    continue\n                # Parse ID line\n                original_id = ''\n                result = regexp_id_line1.search(line)\n                if result is not None:\n                    original_id = result.group(1)\n                result = regexp_id_line2.search(line)\n                if result is not None:\n                    original_id = result.group(1)\n                if original_id and not expecting_title_line:\n                    if 'ModuleID' not in childmetadata:\n                        childmetadata['ModuleID'] = original_id\n                    else:\n                        childmetadata['ConvertedFromID'] = original_id\n                    # An ID line should be followed by a title line\n                    expecting_title_line = True\n                    continue\n                else:\n                    # Failed to match any of the tentative block line types!\n                    # Abort the tentative block parsing\n                    # Put back tentative lines\n                    for tentativeline in tentativecontentlines:\n                        parsedcontentlines.append(tentativeline)\n                    # Switch state\n                    parsing_state = REGULAR_LINES\n                    expecting_title_line = False\n                    childmetadata = {}\n\n\n\n    def _scan_file_for_includes(self, asfile, recursive=False):\n        includedfilelist = []\n        regexp = re.compile(r'^\\s*include::([^\\[]+)\\[[^\\]]*\\]')\n        with open(asfile, 'r') as f:\n            for line in f:\n                result = regexp.search(line)\n                if result is not None:\n                    includedfile = self.context.resolve_raw_attribute_value(result.group(1))\n                    directory = os.path.dirname(asfile)\n                    path_to_included_file = os.path.relpath(os.path.realpath(os.path.normpath(os.path.join(directory, includedfile))))\n                    if includedfile.endswith('.adoc'):\n                        includedfilelist.append(path_to_included_file)\n        allincludedfilelist = includedfilelist\n        if (recursive):\n            for file in includedfilelist:\n                if not os.path.exists(file):\n                    print 'ERROR: While scanning ' + asfile + ': included file, ' + file + ', does not exist'\n                    sys.exit()\n                childincludedfilelist = self._scan_file_for_includes(file, recursive=True)\n                allincludedfilelist.extend(childincludedfilelist)\n        return allincludedfilelist\n\n    def _resolve_includes(self, file, baselevel=0, selectedtags=None):\n        # Resolve all of the nested includes in 'file' to plain text and return a plain text array of all the lines in the file\n        if not os.path.exists(file):\n            print 'ERROR: Include file not found: ' + file\n            sys.exit()\n        if (selectedtags is not None) and (len(selectedtags) > 0):\n            istaggingactive = True\n            showcontent = False\n            currtagname = ''\n        else:\n            istaggingactive = False\n            showcontent = True\n            currtagname = ''\n        linesinfile = []\n        regexp_include = re.compile(r'^\\s*include::([^\\[]+)\\[([^\\]]*)\\]')\n        regexp_title = re.compile(r'^(=+)\\s+(\\S.*)')\n        regexp_tag_begin = re.compile(r'tag::([^\\[]+)\\[\\]')\n        regexp_tag_end   = re.compile(r'end::([^\\[]+)\\[\\]')\n        with open(file, 'r') as f:\n            for line in f:\n                if istaggingactive:\n                    result = regexp_tag_begin.search(line)\n                    if result is not None:\n                        tagname = result.group(1)\n                        # Checks 'currtagname' in order to ignore nested tags\n                        if (not currtagname) and (tagname in selectedtags):\n                            showcontent = True\n                            currtagname = tagname\n                        # Do not include tagged line in output\n                        continue\n                    result = regexp_tag_end.search(line)\n                    if result is not None:\n                        tagname = result.group(1)\n                        if tagname == currtagname:\n                            showcontent = False\n                            currtagname = ''\n                        # Do not include tagged line in output\n                        continue\n                if not showcontent:\n                    # Content is currently tagged off - skip this line\n                    continue\n                result = regexp_title.search(line)\n                if result is not None:\n                    childequalssigncount = len(result.group(1))\n                    title = result.group(2)\n                    linesinfile.append('=' * (childequalssigncount + baselevel) + ' ' + title)\n                    continue\n                result = regexp_include.search(line)\n                if result is not None:\n                    includedfile = self.context.resolve_raw_attribute_value(result.group(1))\n                    options      = result.group(2)\n                    optmap = self._parse_include_opts(options)\n                    childbaselevel = baselevel\n                    if 'leveloffset' in optmap:\n                        leveloffset = optmap['leveloffset'].strip()\n                        if leveloffset.startswith('+'):\n                            childbaselevel = baselevel + int(leveloffset)\n                        else:\n                            childbaselevel = int(leveloffset)\n                    directory = os.path.dirname(file)\n                    path_to_included_file = os.path.relpath(os.path.realpath(os.path.normpath(os.path.join(directory, includedfile))))\n                    taglist = []\n                    if ('tag' in optmap):\n                        taglist.append(optmap['tag'].strip())\n                    if ('tags' in optmap):\n                        taglist.extend(optmap['tags'].split(';'))\n                    linesinfile.extend(self._resolve_includes(path_to_included_file, baselevel=childbaselevel, selectedtags=taglist))\n                    continue\n                linesinfile.append(line)\n        return linesinfile\n\n    def _parse_include_opts(self, optstring):\n        # Returns a map of property, value pairs\n        optlist = optstring.split(',')\n        optmap = {}\n        for opt in optlist:\n            if opt.count('=') > 0:\n                (prop, value) = opt.split('=', 1)\n                optmap[prop.strip().lower()] = value.strip()\n        return optmap\n\n    def _create_from_csv(self,args):\n        csvfile = args.FROM_FILE\n        USING_LEVELS = False\n        with open(csvfile, 'r') as filehandle:\n            # First line should be the column headings\n            headings = filehandle.readline().strip().replace(' ','')\n            headinglist = headings.split(',')\n            # Alias 'NestingLevel' to 'Level'\n            if 'NestingLevel' in headinglist:\n                k = headinglist.index('NestingLevel')\n                headinglist[k] = 'Level'\n            # Alias 'NewModuleID' to 'ModuleID'\n            if 'NewModuleID' in headinglist:\n                k = headinglist.index('NewModuleID')\n                headinglist[k] = 'ModuleID'\n            # Check plausibility of headinglist\n            if ('Category' not in headinglist) or ('ModuleID' not in headinglist):\n                print 'ERROR: CSV file does not have correct format'\n                sys.exit()\n            if 'Level' in headinglist:\n                USING_LEVELS = True\n            # Create initial copy of the generated-master.adoc file\n            MASTERDOC_FILENAME = 'generated-master.adoc'\n            templatefile = os.path.join(self.context.templatePath, 'master.adoc')\n            shutil.copyfile(templatefile, MASTERDOC_FILENAME)\n            # Initialize variables to track level nesting\n            nestedfilestack = []\n            nestedlevelstack = []\n            currentfile = MASTERDOC_FILENAME\n            currentlevel = 0\n            # Read and parse the CSV file\n            completefile = filehandle.read()\n            lines = self.smart_split(completefile, '\\n', preserveQuotes=True)\n            for line in lines:\n                if line.strip() != '':\n                    fieldlist = self.smart_split(line.strip())\n                    metadata = dict(zip(headinglist, fieldlist))\n                    # Skip rows with Implement field set to 'no'\n                    if ('Implement' in metadata) and (metadata['Implement'].lower() == 'no'):\n                        print 'INFO: Skipping unimplemented module/assembly: ' + metadata['ModuleID']\n                        continue\n                    # Weed out irrelevant metadata entries\n                    for field,value in metadata.items():\n                        if field not in self.context.allMetadataFields:\n                            del(metadata[field])\n                    if metadata['Type'] == '':\n                        # Assume it's an empty row (i.e. fields are empty, row is just commas)\n                        if (not USING_LEVELS) and (currentlevel == 1):\n                            # Pop back to level 0\n                            currentfile = nestedfilestack.pop()\n                            currentlevel = nestedlevelstack.pop()\n                        # Skip empty row\n                        continue\n                    # Process modules and assemblies\n                    if USING_LEVELS:\n                        level = int(metadata['Level'])\n                    else:\n                        if (metadata['Type'] == 'assembly'):\n                            # For sheets without levels, assemblies are always level 1\n                            level = 1\n                        else:\n                            # Calculate module level, for a sheet without levels\n                            level = currentlevel + 1\n                    while level <= currentlevel:\n                        # Dig back through the stack to find the parent of this module or assembly\n                        currentfile = nestedfilestack.pop()\n                        currentlevel = nestedlevelstack.pop()\n                    metadata['ParentAssemblies'] = currentfile\n                    newfile = self.context.moduleFactory.create(metadata)\n                    self.add_include_to_assembly(currentfile, newfile, level - currentlevel)\n                    if (metadata['Type'] == 'assembly'):\n                        # Push the assembly onto the level stack\n                        nestedfilestack.append(currentfile)\n                        nestedlevelstack.append(currentlevel)\n                        currentfile = newfile\n                        currentlevel = level\n\n\n    def smart_split(self, line, splitchar=',', preserveQuotes=False):\n        list = []\n        isInQuotes = False\n        currfield = ''\n        for ch in line:\n            if not isInQuotes:\n                if ch == splitchar:\n                    list.append(currfield)\n                    currfield = ''\n                    continue\n                if ch == '\"':\n                    isInQuotes = True\n                    if not preserveQuotes:\n                        continue\n                currfield += ch\n            else:\n                if ch == '\\r' or ch == '\\n':\n                    # Eliminate newlines from quoted fields\n                    continue\n                if ch == '\"':\n                    isInQuotes = False\n                    if not preserveQuotes:\n                        continue\n                currfield += ch\n        # Don't forget to append the last field (if any)!\n        if currfield:\n            list.append(currfield)\n        return list\n\n\n    def book(self,args):\n        if self.context.ASSEMBLIES_DIR == '.' or self.context.MODULES_DIR == '.':\n            print 'ERROR: book command is only usable for a standard directory layout, with defined assemblies and modules directories'\n            sys.exit()\n        if args.create:\n            # Create book and (optionally) add categories\n            self._book_create(args)\n        elif args.category_list:\n            # Add categories\n            self._book_categories(args)\n        else:\n            print 'ERROR: No options specified'\n\n\n    def _book_create(self,args):\n        bookdir = args.BOOK_DIR\n        if os.path.exists(bookdir):\n            print 'ERROR: Book directory already exists: ' + bookdir\n            sys.exit()\n        os.mkdir(bookdir)\n        os.mkdir(os.path.join(bookdir, self.context.ASSEMBLIES_DIR))\n        os.mkdir(os.path.join(bookdir, self.context.MODULES_DIR))\n        os.mkdir(os.path.join(bookdir, self.context.IMAGES_DIR))\n        os.symlink(os.path.join('..', 'shared', 'attributes.adoc'), os.path.join(bookdir, 'attributes.adoc'))\n        os.symlink(\n            os.path.join('..', 'shared', 'attributes-links.adoc'),\n            os.path.join(bookdir, 'attributes-links.adoc')\n        )\n        templatefile = os.path.join(self.context.templatePath, 'master.adoc')\n        shutil.copyfile(templatefile, os.path.join(bookdir, 'master.adoc'))\n        templatefile = os.path.join(self.context.templatePath, 'master-docinfo.xml')\n        shutil.copyfile(templatefile, os.path.join(bookdir, 'master-docinfo.xml'))\n        # Add categories (if specified)\n        if args.category_list:\n            self._book_categories(args)\n\n\n    def _book_categories(self, args):\n        bookdir = args.BOOK_DIR\n        if not os.path.exists(bookdir):\n            print 'ERROR: Book directory does not exist: ' + bookdir\n            sys.exit()\n        imagesdir = os.path.join(bookdir, self.context.IMAGES_DIR)\n        modulesdir = os.path.join(bookdir, self.context.MODULES_DIR)\n        assembliesdir = os.path.join(bookdir, self.context.ASSEMBLIES_DIR)\n        if not os.path.exists(imagesdir):\n            os.mkdir(imagesdir)\n        if not os.path.exists(modulesdir):\n            os.mkdir(modulesdir)\n        if not os.path.exists(assembliesdir):\n            os.mkdir(assembliesdir)\n        categorylist = args.category_list.split(',')\n        map(str.strip, categorylist)\n        for category in categorylist:\n            if not os.path.islink(os.path.join(imagesdir, category)):\n                os.symlink(\n                    os.path.join('..', '..', self.context.IMAGES_DIR, category),\n                    os.path.join(imagesdir, category)\n                )\n            if not os.path.islink(os.path.join(modulesdir, category)):\n                os.symlink(\n                    os.path.join('..', '..', self.context.MODULES_DIR, category),\n                    os.path.join(modulesdir, category)\n                )\n            if not os.path.islink(os.path.join(assembliesdir, category)):\n                os.symlink(\n                    os.path.join('..', '..', self.context.ASSEMBLIES_DIR, category),\n                    os.path.join(assembliesdir, category)\n                )\n\n\n    def update(self,args):\n        if (not args.fix_includes) and (not args.parent_assemblies) and (not args.fix_links) and (not args.generate_ids) and (not args.add_contexts):\n            print 'ERROR: Missing required option(s)'\n            sys.exit()\n        if args.attribute_files:\n            attrfilelist = args.attribute_files.strip().split(',')\n        else:\n            attrfilelist = None\n        if args.FILE:\n            head, tail = os.path.split(args.FILE)\n            type = self.type_of_file(tail)\n            if type == 'assembly':\n                assemblyfiles = [ args.FILE ]\n                modulefiles = []\n            elif type in ['procedure', 'concept', 'reference']:\n                assemblyfiles = []\n                modulefiles = [args.FILE]\n            else:\n                print 'ERROR: File must be a module or an assembly: ' + args.FILE\n                sys.exit()\n        else:\n            # Determine the set of categories to update\n            categoryset = set()\n            if args.category_list:\n                categoryset = set(args.category_list.split(','))\n                map(str.strip, categoryset)\n            elif args.book:\n                if not os.path.exists(args.book):\n                    print 'ERROR: ' + args.book + ' directory does not exist.'\n                    sys.exit()\n                categoryset = self.scan_for_categories(os.path.join(args.book, self.context.MODULES_DIR))\\\n                              | self.scan_for_categories(os.path.join(args.book, self.context.ASSEMBLIES_DIR))\n            else:\n                categoryset = self.scan_for_categories(self.context.MODULES_DIR) | self.scan_for_categories(self.context.ASSEMBLIES_DIR)\n            assemblyfiles = self.scan_for_categorised_files(self.context.ASSEMBLIES_DIR, categoryset, filefilter='assembly')\n            modulefiles = self.scan_for_categorised_files(self.context.MODULES_DIR, categoryset, filefilter='module')\n            imagefiles = self.scan_for_categorised_files(self.context.IMAGES_DIR, categoryset)\n        # Select the kind of update to implement\n        if args.fix_includes:\n            self._update_fix_includes(assemblyfiles, modulefiles)\n        if args.fix_links:\n            self._update_fix_links(assemblyfiles, modulefiles, attrfilelist)\n        if args.parent_assemblies:\n            self._update_parent_assemblies(assemblyfiles)\n        if args.generate_ids:\n            self._update_generate_ids(assemblyfiles, modulefiles)\n        if args.add_contexts:\n            self._add_contexts(assemblyfiles, modulefiles, attrfilelist, args)\n\n\n    def scan_for_categories(self, rootdir):\n        categoryset = set()\n        cwd = os.getcwd()\n        os.chdir(rootdir)\n        for root, dirs, files in os.walk(os.curdir, followlinks=True):\n            for dir in dirs:\n                categoryset.add(os.path.normpath(os.path.join(root, dir)))\n        os.chdir(cwd)\n        # Add the empty category to the category set\n        categoryset.add('')\n        return categoryset\n\n\n    def scan_for_categorised_files(self, rootdir, categoryset, filefilter=None):\n        filelist = []\n        for category in categoryset:\n            categorydir = os.path.join(rootdir, category)\n            if os.path.exists(categorydir):\n                for entry in os.listdir(categorydir):\n                    pathname = os.path.join(rootdir, category, entry)\n                    if os.path.isfile(pathname):\n                        if filefilter is None:\n                            filelist.append(pathname)\n                        elif filefilter == 'assembly' and self.type_of_file(entry) == 'assembly':\n                            filelist.append(pathname)\n                        elif filefilter == 'module' and self.type_of_file(entry) in ['module', 'concept', 'procedure', 'reference']:\n                            filelist.append(pathname)\n        return filelist\n\n\n    def _update_fix_includes(self, assemblyfiles, modulefiles):\n        # Create dictionaries mapping norm(filename) -> [pathname, pathname, ...]\n        assemblyfiledict = {}\n        for filepath in assemblyfiles:\n            head, tail = os.path.split(filepath)\n            normfilename = self.context.moduleFactory.normalize_filename(tail)\n            if normfilename not in assemblyfiledict:\n                assemblyfiledict[normfilename] = [filepath]\n            else:\n                assemblyfiledict[normfilename].append(filepath)\n        modulefiledict = {}\n        for filepath in modulefiles:\n            head, tail = os.path.split(filepath)\n            normfilename = self.context.moduleFactory.normalize_filename(tail)\n            if normfilename not in modulefiledict:\n                modulefiledict[normfilename] = [filepath]\n            else:\n                modulefiledict[normfilename].append(filepath)\n        # Scan and update include directives in assembly files\n        for assemblyfile in assemblyfiles:\n            self._update_include_directives(assemblyfile, assemblyfiledict, modulefiledict)\n\n\n    def _update_include_directives(self, file, assemblyfiledict, modulefiledict):\n        print 'Updating include directives for file: ' + file\n        regexp = re.compile(r'^\\s*include::([^\\[\\{]+)\\[([^\\]]*)\\]')\n        dirname = os.path.dirname(file)\n        # Create temp file\n        fh, abs_path = tempfile.mkstemp()\n        with os.fdopen(fh, 'w') as new_file:\n            with open(file) as old_file:\n                for line in old_file:\n                    if line.lstrip().startswith('include::'):\n                        #print '\\t' + line.strip()\n                        result = regexp.search(line)\n                        if result is not None:\n                            includepath = result.group(1)\n                            testpath = os.path.normpath(os.path.join(dirname, includepath))\n                            if not os.path.exists(testpath):\n                                includedir, includefile = os.path.split(includepath)\n                                normincludefile = self.context.moduleFactory.normalize_filename(includefile)\n                                if self.type_of_file(normincludefile) == 'assembly':\n                                    # Assembly case\n                                    if normincludefile in assemblyfiledict:\n                                        pathlist = assemblyfiledict[normincludefile]\n                                        new_includepath = self.choose_includepath(dirname, pathlist)\n                                        if new_includepath is not None:\n                                            new_file.write('include::' + new_includepath + '[' + result.group(2) + ']\\n')\n                                            print 'Replacing: ' + includepath + ' with ' + new_includepath\n                                            continue\n                                else:\n                                    # Module case\n                                    if normincludefile in modulefiledict:\n                                        pathlist = modulefiledict[normincludefile]\n                                        new_includepath = self.choose_includepath(dirname, pathlist)\n                                        if new_includepath is not None:\n                                            new_file.write('include::' + new_includepath + '[' + result.group(2) + ']\\n')\n                                            print 'Replacing: ' + includepath + ' with ' + new_includepath\n                                            continue\n                        else:\n                            print 'WARN: Unparsable include:' + line.strip()\n                    new_file.write(line)\n        # Remove original file\n        os.remove(file)\n        # Move new file\n        shutil.move(abs_path, file)\n\n\n    def choose_includepath(self, basedir, pathlist):\n        if len(pathlist) == 1:\n            return os.path.relpath(pathlist[0], basedir)\n        else:\n            print '\\tChoose the correct path for the included file or S to skip:'\n            for k, path in enumerate(pathlist):\n                print '\\t' + str(k) + ') ' + path\n            print '\\tS) Skip and leave this include unchanged'\n            response = ''\n            while response.strip() == '':\n                response = raw_input('\\tEnter selection [S]: ')\n                response = response.strip()\n                if (response == '') or (response.lower() == 's'):\n                    # Skip\n                    return None\n                elif (0 <= int(response)) and (int(response) < len(pathlist)):\n                    return os.path.relpath(pathlist[int(response)], basedir)\n                else:\n                    response = ''\n            return None\n\n\n    def _scan_for_parent_assemblies(self, assemblylist):\n        # Create dictionary of modules included by assemblies\n        assemblyincludes = {}\n        for assemblyfile in assemblylist:\n            assemblyincludes[assemblyfile] = self._scan_file_for_includes(assemblyfile)\n        # print assemblyincludes\n        # Invert dictionary\n        parentassemblies = {}\n        for assemblyfile in assemblyincludes:\n            for modulefile in assemblyincludes[assemblyfile]:\n                if modulefile not in parentassemblies:\n                    parentassemblies[modulefile] = [assemblyfile]\n                else:\n                    parentassemblies[modulefile].append(assemblyfile)\n        return parentassemblies, assemblyincludes\n\n\n    def _update_parent_assemblies(self, assemblylist):\n        parentassemblies, assemblyincludes = self._scan_for_parent_assemblies(assemblylist)\n        # Update the ParentAssemblies metadata in each of the module files\n        metadata = {}\n        for modulefile in parentassemblies:\n            metadata['ParentAssemblies'] = ','.join(parentassemblies[modulefile])\n            self.update_metadata(modulefile, metadata)\n\n    def _scan_for_bookfiles(self):\n        # Scan current dir for top-level book files\n        booklist = []\n        for root, dirs, files in os.walk(os.curdir):\n            for dir in dirs:\n                # Test for existence of master.adoc file\n                bookdir = os.path.normpath(os.path.join(root, dir))\n                bookfile = os.path.join(bookdir, 'master.adoc')\n                if os.path.exists(bookfile):\n                    booklist.append(bookfile)\n        return booklist\n\n    def _update_fix_links(self, assemblyfiles, modulefiles, attrfilelist = None):\n        # Set of files whose links should be fixed\n        fixfileset = set(assemblyfiles) | set(modulefiles)\n        # Identify top-level book files to scan\n        booklist = self._scan_for_bookfiles()\n        # Initialize anchor ID dictionary, legacy ID, and root of ID lookup\n        anchorid_dict = {}\n        legacyid_dict = {}\n        rootofid_dict = {}\n        # Process each book in the list\n        for bookfile in booklist:\n            booktitle = self._scan_for_title(bookfile)\n            booktitle_slug = self._convert_title_to_slug(booktitle)\n            #print 'Title URL slug: ' + booktitle_slug\n            print 'Title: ' + booktitle\n            self.context.clear_attributes()\n            anchorid_dict, legacyid_dict, rootofid_dict = self._parse_file_for_anchorids(anchorid_dict, legacyid_dict, rootofid_dict, booktitle_slug, bookfile)\n            #print anchorid_dict.keys()\n        #print anchorid_dict\n        self.anchorid_dict = anchorid_dict\n        self.legacyid_dict = legacyid_dict\n        self.rootofid_dict = rootofid_dict\n\n        # Generate parentassemblies dictionary for all assemblies\n        categoryset = self.scan_for_categories(self.context.ASSEMBLIES_DIR)\n        assemblyfiles = self.scan_for_categorised_files(self.context.ASSEMBLIES_DIR, categoryset, filefilter='assembly')\n        assemblyfiles.extend(booklist)\n        parentassemblies, assemblyincludes = self._scan_for_parent_assemblies(assemblyfiles)\n        self.parentassemblies = parentassemblies\n\n        for fixfile in fixfileset:\n            print 'Updating links for file: ' + fixfile\n            dirname = os.path.dirname(fixfile)\n            # Create temp file\n            fh, abs_path = tempfile.mkstemp()\n            with os.fdopen(fh, 'w') as new_file:\n                with open(fixfile) as old_file:\n                    for line in old_file:\n                        # Smuggle the 'fixfile' value into the _on_match_*() functions\n                        self._on_match_fixfile = fixfile\n                        line = self._regexp_replace_angles(line)\n                        line = self._regexp_replace_xref(line)\n                        line = self._regexp_replace_link(line)\n                        new_file.write(line)\n            # Remove original file\n            os.remove(fixfile)\n            # Move new file\n            shutil.move(abs_path, fixfile)\n\n\n    def _regexp_replace_angles(self, value):\n        regexp = re.compile(r'<<([^,>]+),?([^>]*)>>')\n        new_value = regexp.sub(self._on_match_xref, value)\n        return new_value\n\n\n    def _regexp_replace_xref(self, value):\n        regexp = re.compile(r'xref:([\\w\\-]+)\\[([^\\]]*)\\]')\n        new_value = regexp.sub(self._on_match_xref, value)\n        return new_value\n\n\n    def _on_match_xref(self, match_obj):\n        anchorid = match_obj.group(1)\n        optionaltext = match_obj.group(2)\n        new_anchorid = self._repair_anchorid(anchorid, self._on_match_fixfile)\n        if optionaltext:\n            return 'xref:' + new_anchorid + '[' + optionaltext + ']'\n        else:\n            return 'xref:' + new_anchorid + '[]'\n\n    def _regexp_replace_link(self, value):\n        regexp = re.compile(r'link:(\\{[\\w\\-]+\\})#([^\\[]+)\\[([^\\]]*)\\]')\n        new_value = regexp.sub(self._on_match_link, value)\n        return new_value\n\n    def _on_match_link(self, match_obj):\n        bookattribute = match_obj.group(1)\n        anchorid = match_obj.group(2)\n        optionaltext = match_obj.group(3)\n        new_anchorid = self._repair_anchorid(anchorid, self._on_match_fixfile)\n        if optionaltext:\n            return 'link:' + bookattribute + '#' + new_anchorid + '[' + optionaltext + ']'\n        else:\n            return 'link:' + bookattribute + '#' + new_anchorid + '[]'\n\n    def _repair_anchorid(self, anchorid, fixfile):\n        if anchorid.endswith('_{context}'):\n            plainanchorid = anchorid.replace('_{context}','')\n        else:\n            plainanchorid = anchorid\n        if plainanchorid in self.anchorid_dict:\n            target_anchorid = plainanchorid\n        elif plainanchorid in self.legacyid_dict:\n            target_anchorid = self.legacyid_dict[plainanchorid]\n        elif plainanchorid in self.rootofid_dict:\n            target_anchorid = self.choose_anchorid_from_rootofid_dict(plainanchorid)\n            if target_anchorid is None:\n                # Leave the ID unchanged\n                target_anchorid = anchorid\n        elif '_' in plainanchorid:\n                # Last attempt to fix - ID might have wrong context value after the '_' char\n                rootofid, contextval = plainanchorid.rsplit('_', 1)\n                if rootofid in self.rootofid_dict:\n                    target_anchorid = self.choose_anchorid_from_rootofid_dict(rootofid)\n                    if target_anchorid is None:\n                        # Leave the ID unchanged\n                        target_anchorid = anchorid\n                else:\n                    target_anchorid = anchorid\n        else:\n            print 'WARNING: link to unknown ID: ' + anchorid\n            target_anchorid = anchorid\n\n        # Special case: if the file containing the xref and the file containing the target ID have the *same* parent assembly,\n        #   then the ID in the xref *should* use _{context} (this facilitates content sharing between products)\n        if '_' in target_anchorid:\n            rootofid, contextval = target_anchorid.rsplit('_', 1)\n            use_context_suffix = False\n            for parent in self.parentassemblies[fixfile]:\n                if target_anchorid in self.anchorid_dict:\n                    for booktitle_slug in self.anchorid_dict[target_anchorid]:\n                        targetfile = self.anchorid_dict[target_anchorid][booktitle_slug]['FilePath']\n                        if (targetfile.startswith(self.context.MODULES_DIR)) and (parent in self.parentassemblies[targetfile]):\n                            use_context_suffix = True\n            if use_context_suffix:\n                target_anchorid = rootofid + '_{context}'\n        return target_anchorid\n\n    def choose_anchorid_from_rootofid_dict(self, anchorid):\n        idlist = self.rootofid_dict[anchorid]\n        if len(idlist) == 1:\n            return idlist[0]\n        else:\n            print '\\tChoose the correct target ID for the link or S to skip:'\n            for k, targetid in enumerate(idlist):\n                print '\\t' + str(k) + ') ' + targetid\n            print '\\tS) Skip and leave this include unchanged'\n            response = ''\n            while response.strip() == '':\n                response = raw_input('\\tEnter selection [S]: ')\n                response = response.strip()\n                if (response == '') or (response.lower() == 's'):\n                    # Skip\n                    return None\n                elif (0 <= int(response)) and (int(response) < len(idlist)):\n                    return idlist[int(response)]\n                else:\n                    response = ''\n            return None\n\n    def _scan_for_title(self, filepath):\n        if not os.path.exists(filepath):\n            print 'ERROR: _scan_for_title: No such file: ' + filepath\n            sys.exit()\n        rawtitle = ''\n        regexp = re.compile(r'^=\\s+(\\S.*)')\n        with open(filepath, 'r') as f:\n            for line in f:\n                result = regexp.search(line)\n                if result is not None:\n                    rawtitle = result.group(1)\n                    break\n            if rawtitle == '':\n                print 'ERROR: _scan_for_title: No title found in file: ' + filepath\n                sys.exit()\n        return self.context.resolve_raw_attribute_value(rawtitle)\n\n\n    def _convert_title_to_slug(self, title):\n        return title.strip().lower().replace(' ', '_').replace('-', '_')\n\n\n    def _parse_file_for_anchorids(self, anchorid_dict, legacyid_dict, rootofid_dict, booktitle_slug, filepath):\n        # Define action enums\n        NO_ACTION = 0\n        ORDINARY_LINE = 1\n        METADATA_LINE = 2\n        ID_LINE = 3\n        TITLE_LINE = 4\n        INCLUDE_LINE = 5\n        ATTRIBUTE_LINE = 6\n        BLANK_LINE = 7\n\n        # Define regular expressions\n        regexp_metadata = re.compile(r'^\\s*//\\s*(\\w+)\\s*:\\s*(.+)')\n        regexp_id_line1 = re.compile(r'\\[\\[\\s*(\\S+)\\s*\\]\\]')\n        regexp_id_line2 = re.compile(r'\\[id\\s*=\\s*[\\'\"]\\s*(\\S+)\\s*[\\'\"]\\]')\n        regexp_title = re.compile(r'^(=+)\\s+(\\S.*)')\n        regexp_attribute = re.compile(r'^:([\\w\\-]+):\\s+(.*)')\n        regexp_include = re.compile(r'^\\s*include::([^\\[]+)\\[([^\\]]*)\\]')\n        regexp_blank = re.compile(r'^\\s*$')\n\n        if not os.path.exists(filepath):\n            print 'ERROR: _parse_file_for_anchorids: File does not exist: ' + filepath\n            sys.exit()\n        with open(filepath, 'r') as filehandle:\n            tentative_metadata = {}\n            tentative_anchor_id = ''\n            for line in filehandle:\n                action = NO_ACTION\n                # Parse the current line\n                while action == NO_ACTION:\n                    result = regexp_metadata.search(line)\n                    if result is not None:\n                        property = result.group(1)\n                        value = result.group(2)\n                        action = METADATA_LINE\n                        continue\n                    result = regexp_id_line1.search(line)\n                    if result is not None:\n                        rawanchorid = result.group(1)\n                        action = ID_LINE\n                        continue\n                    result = regexp_id_line2.search(line)\n                    if result is not None:\n                        rawanchorid = result.group(1).strip()\n                        action = ID_LINE\n                        continue\n                    result = regexp_title.search(line)\n                    if result is not None:\n                        rawtitle = result.group(2)\n                        title = self.context.resolve_raw_attribute_value(rawtitle)\n                        action = TITLE_LINE\n                        continue\n                    result = regexp_attribute.search(line)\n                    if result is not None:\n                        name = result.group(1)\n                        value = result.group(2).strip()\n                        self.context.update_attribute(name, value)\n                        action = ATTRIBUTE_LINE\n                        continue\n                    result = regexp_include.search(line)\n                    if result is not None:\n                        rawincludefile = result.group(1)\n                        includefile = self.context.resolve_raw_attribute_value(rawincludefile)\n                        action = INCLUDE_LINE\n                        continue\n                    result = regexp_blank.search(line)\n                    if result is not None:\n                        action = BLANK_LINE\n                        continue\n                    # Default action is ordinary line\n                    action = ORDINARY_LINE\n                # Take action\n                if action == BLANK_LINE or action == ATTRIBUTE_LINE:\n                    # It's a noop\n                    pass\n                elif (action == ORDINARY_LINE) and tentative_anchor_id:\n                    # Define an anchor ID that is not associated with a heading\n                    if tentative_anchor_id not in anchorid_dict:\n                        # Initialize the sub-dictionary\n                        anchorid_dict[tentative_anchor_id] = {}\n                    if booktitle_slug in anchorid_dict[tentative_anchor_id]:\n                        print 'WARNING: Anchor ID: ' + tentative_anchor_id + 'appears more than once in book: ' + booktitle_slug\n                    else:\n                        anchorid_dict[tentative_anchor_id][booktitle_slug] = { 'FilePath': os.path.relpath(os.path.realpath(filepath)) }\n                    tentative_anchor_id = ''\n                    tentative_root_of_id = ''\n                    tentative_context_of_id = None\n                    tentative_metadata = {}\n                elif action == ORDINARY_LINE:\n                    # After hitting an ordinary line, preceding metadata is no longer current\n                    tentative_metadata = {}\n                elif action == METADATA_LINE:\n                    if property in self.context.optionalMetadataFields:\n                        tentative_metadata[property] = value\n                elif action == ID_LINE:\n                    if rawanchorid.endswith('}'):\n                        currentcontext = self.context.lookup_attribute('context')\n                        if currentcontext is not None:\n                            anchorid = rawanchorid.replace('{context}', currentcontext)\n                            rootofid = rawanchorid.replace('_{context}', '')\n                        else:\n                            print 'ERROR: Found ID with embedded {context}, but no context attribute defined'\n                            print '    file: ' + filepath\n                            print '    ID:   ' + rawanchorid\n                            sys.exit()\n                    else:\n                        anchorid = rawanchorid\n                        rootofid = rawanchorid\n                        currentcontext = None\n                    tentative_anchor_id = anchorid\n                    tentative_root_of_id = rootofid\n                    tentative_context_of_id = currentcontext\n                elif (action == TITLE_LINE) and tentative_anchor_id:\n                    # Define an anchor ID that is associated with a heading\n                    if tentative_anchor_id not in anchorid_dict:\n                        # Initialize the sub-dictionary\n                        anchorid_dict[tentative_anchor_id] = {}\n                    if booktitle_slug in anchorid_dict[tentative_anchor_id]:\n                        print 'WARNING: Anchor ID: ' + tentative_anchor_id + 'appears more than once in book: ' + booktitle_slug\n                    else:\n                        anchorid_dict[tentative_anchor_id][booktitle_slug] = { 'FilePath': os.path.relpath(os.path.realpath(filepath)), 'Title': title, 'Context': tentative_context_of_id }\n                        if 'ConvertedFromID' in tentative_metadata:\n                            anchorid_dict[tentative_anchor_id][booktitle_slug]['ConvertedFromID'] = tentative_metadata['ConvertedFromID']\n                            legacyid_dict[tentative_metadata['ConvertedFromID']] = tentative_anchor_id\n                        if tentative_root_of_id != tentative_anchor_id:\n                            if tentative_root_of_id not in rootofid_dict:\n                                # Initialize list of anchor IDs in this slot\n                                rootofid_dict[tentative_root_of_id] = [ tentative_anchor_id ]\n                            else:\n                                rootofid_dict[tentative_root_of_id].append(tentative_anchor_id)\n                    tentative_anchor_id = ''\n                    tentative_root_of_id = ''\n                    tentative_context_of_id = None\n                    tentative_metadata = {}\n                elif (action == TITLE_LINE) and not tentative_anchor_id:\n                    tentative_anchor_id = ''\n                    tentative_root_of_id = ''\n                    tentative_context_of_id = None\n                    tentative_metadata = {}\n                elif action == INCLUDE_LINE:\n                    currentdir, basename = os.path.split(filepath)\n                    includefile = os.path.normpath(os.path.join(currentdir, includefile))\n                    if not os.path.exists(includefile):\n                        print 'ERROR: Included file does not exist: ' + includefile\n                        sys.exit()\n                    anchorid_dict, legacyid_dict, rootofid_dict = self._parse_file_for_anchorids(anchorid_dict, legacyid_dict, rootofid_dict, booktitle_slug, includefile)\n                    tentative_anchor_id = ''\n                    tentative_root_of_id = ''\n                    tentative_context_of_id = None\n                    tentative_metadata = {}\n        return anchorid_dict, legacyid_dict, rootofid_dict\n\n    def _update_generate_ids(self, assemblyfiles, modulefiles):\n        # Set of files for which IDs should be generated\n        fixfileset = set(assemblyfiles) | set(modulefiles)\n\n        # Define regular expressions\n        regexp_id_line1 = re.compile(r'^\\s*\\[\\[\\s*(\\S+)\\s*\\]\\]\\s*$')\n        regexp_id_line2 = re.compile(r'^\\s*\\[id\\s*=\\s*[\\'\"]\\s*(\\S+)\\s*[\\'\"]\\]\\s*$')\n        regexp_title = re.compile(r'^(=+)\\s+(\\S.*)')\n\n        for fixfile in fixfileset:\n            print 'Adding missing IDs to file: ' + fixfile\n            dirname, basename = os.path.split(os.path.normpath(fixfile))\n            idprefix = dirname.replace(os.sep, '-').replace('_', '-') + '-' + self.moduleid_of_file(basename)\n            # Create temp file\n            fh, abs_path = tempfile.mkstemp()\n            with os.fdopen(fh, 'w') as new_file:\n                with open(fixfile) as old_file:\n                    prevline = ''\n                    newidlist = []\n                    disambig_suffix = 1\n                    for line in old_file:\n                        if (regexp_title.search(line) is not None)\\\n                                and (regexp_id_line1.search(prevline) is None)\\\n                                and (regexp_id_line2.search(prevline) is None):\n                            # Parse title line\n                            result = regexp_title.search(line)\n                            title = result.group(2)\n                            # Insert module ID\n                            newid = idprefix + '-' + self.title_to_id(title)\n                            if newid in newidlist:\n                                newid = newid + '-' + '{0:0>3}'.format(disambig_suffix)\n                                disambig_suffix += 1\n                            newidlist.append(newid)\n                            new_file.write('[id=\"' + newid + '\"]\\n')\n                        new_file.write(line)\n                        prevline = line\n            # Remove original file\n            os.remove(fixfile)\n            # Move new file\n            shutil.move(abs_path, fixfile)\n\n\n    def update_metadata(self, file, metadata):\n        print 'Updating metadata for file: ' + file\n        regexp = re.compile(r'^\\s*//\\s*(\\w+)\\s*:.*')\n        # Scan file for pre-existing metadata settings\n        preexisting = set()\n        with open(file) as scan_file:\n            for line in scan_file:\n                # Detect end of metadata section\n                if line.startswith('='):\n                    break\n                result = regexp.search(line)\n                if result is not None:\n                    metaname = result.group(1)\n                    if metaname in self.context.optionalMetadataFields:\n                        preexisting.add(metaname)\n        properties2add = (set(metadata.keys()) & self.context.optionalMetadataFields) - preexisting\n        properties2update = set(metadata.keys()) & self.context.optionalMetadataFields & preexisting\n        # Create temp file\n        fh, abs_path = tempfile.mkstemp()\n        with os.fdopen(fh, 'w') as new_file:\n            with open(file) as old_file:\n                START_OF_METADATA = False\n                END_OF_METADATA = False\n                NEW_PROPERTIES_ADDED = False\n                for line in old_file:\n                    # Detect start of metadata section\n                    if line.startswith('// Metadata'):\n                        new_file.write(line)\n                        START_OF_METADATA = True\n                        continue\n                    # Detect end of metadata section\n                    if line.startswith('='):\n                        END_OF_METADATA = True\n                    if START_OF_METADATA and not END_OF_METADATA:\n                        if not NEW_PROPERTIES_ADDED:\n                            for metaname in properties2add:\n                                new_file.write('// ' + metaname + ': ' + metadata[metaname] + '\\n')\n                            NEW_PROPERTIES_ADDED = True\n                        result = regexp.search(line)\n                        if result is not None:\n                            metaname = result.group(1)\n                            if metaname in properties2update:\n                                new_file.write('// ' + metaname + ': ' + metadata[metaname] + '\\n')\n                                continue\n                    new_file.write(line)\n        # Remove original file\n        os.remove(file)\n        # Move new file\n        shutil.move(abs_path, file)\n\n    def orphan_search(self, args):\n        # Determine the set of categories to filter (if any)\n        if args.category_list:\n            filtercategoryset = set(args.category_list.split(','))\n            map(str.strip, filtercategoryset)\n        else:\n            filtercategoryset = None\n        # Parse the specified attributes files (if any)\n        if args.attribute_files:\n            attrfilelist = args.attribute_files.strip().split(',')\n            self.context.parse_attribute_files(attrfilelist)\n        booklist = self._scan_for_bookfiles()\n        # Find the set of all included files\n        allincludedfileset = set()\n        for bookfile in booklist:\n            includedfiles = self._scan_file_for_includes(bookfile, recursive=True)\n            allincludedfileset |= set(includedfiles)\n        # Find the set of all known module and assemblyfiles\n        categoryset = self.scan_for_categories(self.context.MODULES_DIR) | self.scan_for_categories(self.context.ASSEMBLIES_DIR)\n        if filtercategoryset is not None:\n            categoryset &= filtercategoryset\n        assemblyfiles = self.scan_for_categorised_files(self.context.ASSEMBLIES_DIR, categoryset, filefilter='assembly')\n        modulefiles = self.scan_for_categorised_files(self.context.MODULES_DIR, categoryset, filefilter='module')\n        #imagefiles = self.scan_for_categorised_files(self.context.IMAGES_DIR, categoryset)\n        orphanassemblyfiles = set(assemblyfiles) - allincludedfileset\n        orphanmodulefiles   = set(modulefiles) - allincludedfileset\n        # Report\n        for orphanassemblyfile in orphanassemblyfiles:\n            print orphanassemblyfile\n        for orphanmodulefile in orphanmodulefiles:\n            print orphanmodulefile\n\n\n    def mv(self, args):\n        frompattern = os.path.normpath(args.FROM_FILE)\n        topattern = os.path.normpath(args.TO_FILE)\n        # Move each file\n        if frompattern.find('{}') == -1:\n            # No glob patterns => move a single file\n            # Generate a database of parent assemblies\n            categoryset = self.scan_for_categories(self.context.ASSEMBLIES_DIR)\n            assemblyfiles = self.scan_for_categorised_files(self.context.ASSEMBLIES_DIR, categoryset)\n            bookfiles = glob.glob('*/master.adoc')\n            parentassemblies, assemblyincludes = self._scan_for_parent_assemblies(assemblyfiles + bookfiles)\n            self._mv_single_file(parentassemblies, fromfile=frompattern, tofile=topattern)\n        elif frompattern.count('{}') != 1:\n            print 'ERROR: More than one glob pattern {} is not allowed in FROM_FILE'\n            sys.exit()\n        elif topattern.count('{}') != 1:\n            print 'ERROR: TO_FILE must contain a {} substitution pattern'\n            sys.exit()\n        else:\n            fromprefix, fromsuffix = frompattern.split('{}')\n            fromprefixlen = len(fromprefix)\n            fromsuffixlen = len(fromsuffix)\n            fromfiles = glob.glob(frompattern.replace('{}', '*'))\n            for fromfile in fromfiles:\n                if fromsuffixlen==0:\n                    fromfilling = fromfile[fromprefixlen :]\n                else:\n                    fromfilling = fromfile[fromprefixlen : -fromsuffixlen]\n                toprefix, tosuffix = topattern.split('{}')\n                tofile = toprefix + fromfilling + tosuffix\n                # Generate a database of parent assemblies\n                categoryset = self.scan_for_categories(self.context.ASSEMBLIES_DIR)\n                assemblyfiles = self.scan_for_categorised_files(self.context.ASSEMBLIES_DIR, categoryset)\n                bookfiles = glob.glob('*/master.adoc')\n                parentassemblies, assemblyincludes = self._scan_for_parent_assemblies(assemblyfiles + bookfiles)\n                self._mv_single_file(parentassemblies, fromfile, tofile)\n\n\n    def _mv_single_file(self, parentassemblies, fromfile, tofile):\n        # Perform basic sanity checks\n        if not os.path.exists(fromfile):\n            print 'WARN: Origin file does not exist (skipping): ' + fromfile\n            return\n        if os.path.exists(tofile):\n            print 'WARN: File already exists at destination (skipping)' + tofile\n            return\n        # Make sure that the destination directory exists\n        destination_dir, basename = os.path.split(tofile)\n        if destination_dir!='' and not os.path.exists(destination_dir):\n            os.makedirs(destination_dir)\n        # Move the file\n        os.rename(fromfile, tofile)\n        # Update the affected 'include' directives in other files\n        if parentassemblies.has_key(fromfile):\n            for parentassembly in parentassemblies[fromfile]:\n                self._rename_included_file(parentassembly, fromfile, tofile)\n\n\n    def _rename_included_file(self, file, fromfile, tofile):\n        # Ignore include paths with attribute substitutions\n        regexp = re.compile(r'^\\s*include::([^\\[\\{]+)\\[([^\\]]*)\\]')\n        dirname, basename = os.path.split(file)\n        # Create temp file\n        fh, abs_path = tempfile.mkstemp()\n        with os.fdopen(fh, 'w') as new_file:\n            with open(file) as old_file:\n                for line in old_file:\n                    if line.lstrip().startswith('include::'):\n                        result = regexp.search(line)\n                        if result is not None:\n                            includepath = result.group(1)\n                            # Compute unique relative path, factoring out any symbolic links\n                            testpath = os.path.relpath(os.path.realpath(os.path.normpath(os.path.join(dirname, includepath))))\n                            if testpath == os.path.normpath(fromfile):\n                                if basename == 'master.adoc':\n                                    newincludepath = tofile\n                                else:\n                                    newincludepath = os.path.relpath(tofile, dirname)\n                                new_file.write('include::' + newincludepath + '[' + result.group(2) + ']\\n')\n                                continue\n                    new_file.write(line)\n        # Remove original file\n        os.remove(file)\n        # Move new file\n        shutil.move(abs_path, file)\n\n\n    def _add_contexts(self, assemblyfiles, modulefiles, attrfilelist, args):\n        # Set of files to which contexts should be added\n        fixfileset = set(assemblyfiles) | set(modulefiles)\n        # Parse the specified attributes files\n        if attrfilelist is not None:\n            self.context.parse_attribute_files(attrfilelist)\n        else:\n            print 'WARNING: No attribute files specified'\n\n        # Define some enums for state machine\n        REGULAR_LINES = 0\n        EXPECTING_CONTEXT_SET = 1\n        EXPECTING_INCLUDE = 2\n        EXPECTING_CONTEXT_RESTORE = 3\n        LEGACY_MODE = 4\n\n        # Define regular expressions\n        regexp_id_line1 = re.compile(r'^\\s*\\[\\[\\s*(\\S+)\\s*\\]\\]\\s*$')\n        regexp_id_line2 = re.compile(r'^\\s*\\[id\\s*=\\s*[\\'\"]\\s*(\\S+)\\s*[\\'\"]\\]\\s*$')\n        regexp_title = re.compile(r'^(=+)\\s+(\\S.*)')\n\n        for fixfile in fixfileset:\n            print 'Adding contexts to file: ' + fixfile\n            # Initialize Boolean state variables\n            parsing_state = REGULAR_LINES\n            # Initialize loop variables\n            title = ''\n            most_recent_root_id = ''\n            title_id = ''\n            title_id_sha = ''\n            if fixfile in assemblyfiles:\n                is_assembly = True\n            else:\n                is_assembly = False\n            dirname = os.path.dirname(fixfile)\n            # Create temp file\n            fh, abs_path = tempfile.mkstemp()\n            with os.fdopen(fh, 'w') as new_file:\n                with open(fixfile) as old_file:\n                    for line in old_file:\n                        # Ignore blank lines\n                        if line.strip() == '':\n                            new_file.write(line)\n                            continue\n                        # Ignore comment lines\n                        if line.strip().startswith('//'):\n                            new_file.write(line)\n                            continue\n                        # Detect legacy context files (not added using Nebel)\n                        if line.startswith(':parent-context:') or line.startswith('ifdef::context[:parent-context:'):\n                            parsing_state = LEGACY_MODE\n                            break\n                        if parsing_state == REGULAR_LINES:\n                            # Process *unexpected* context definition - signals legacy mode!\n                            if line.startswith(':context:'):\n                                parsing_state = LEGACY_MODE\n                                break\n                            # Process ID line\n                            found_id = ''\n                            result = regexp_id_line1.search(line)\n                            if result is not None:\n                                found_id = result.group(1)\n                            result = regexp_id_line2.search(line)\n                            if result is not None:\n                                found_id = result.group(1)\n                            if found_id:\n                                most_recent_root_id = found_id.replace('_{context}', '')\n                                # Add _{context} to ID\n                                if not found_id.endswith('_{context}'):\n                                    line = line.replace(found_id, found_id + '_{context}')\n                                new_file.write(line)\n                                continue\n                            # Process title line\n                            result = regexp_title.search(line)\n                            if result is not None:\n                                equalssigncount = len(result.group(1))\n                                title = self.context.resolve_raw_attribute_value(result.group(2))\n                                if most_recent_root_id != '':\n                                    title_id = most_recent_root_id\n                                    title_id_sha = self._generate_hash(title_id)\n                                    if args.hash_contexts:\n                                        ctx_segment = title_id_sha\n                                    else:\n                                        ctx_segment = title_id\n                                else:\n                                    print 'ERROR: Expected ID definition before heading = ' + title\n                                    sys.exit()\n                                new_file.write(line)\n                                continue\n                            # Process include:: line\n                            if is_assembly and line.startswith('include::'):\n                                if title_id_sha:\n                                    new_file.write(':parent-of-context-' + title_id_sha + ': {context}\\n')\n                                    new_file.write(':context: {context}-' + ctx_segment + '\\n')\n                                    new_file.write(line)\n                                    new_file.write(':context: {parent-of-context-' + title_id_sha + '}\\n')\n                                    continue\n                                else:\n                                    print 'ERROR: Expected assembly title before first include'\n                                    sys.exit()\n                            # Process :parent-of-context-<SHA>: {context} line\n                            if is_assembly and line.startswith(':parent-of-context-'):\n                                if title_id_sha:\n                                    parsing_state = EXPECTING_CONTEXT_SET\n                                    new_file.write(':parent-of-context-' + title_id_sha + ': {context}\\n')\n                                    continue\n                                else:\n                                    print 'ERROR: Expected assembly title before first instance of :parent-of-context-<SHA>:'\n                                    sys.exit()\n                            # Process regular line\n                            new_file.write(line)\n                            continue\n                        elif parsing_state == EXPECTING_CONTEXT_SET:\n                            # Process :context: {context}-<SEGMENT> line\n                            if line.startswith(':context:'):\n                                parsing_state = EXPECTING_INCLUDE\n                                new_file.write(':context: {context}-' + ctx_segment + '\\n')\n                                continue\n                            else:\n                                print 'ERROR: Expected context definition'\n                                sys.exit()\n                        elif parsing_state == EXPECTING_INCLUDE:\n                            # Process include:: line\n                            if line.startswith('include::'):\n                                parsing_state = EXPECTING_CONTEXT_RESTORE\n                                new_file.write(line)\n                                continue\n                            else:\n                                print 'ERROR: Expected include line'\n                                sys.exit()\n                        elif parsing_state == EXPECTING_CONTEXT_RESTORE:\n                            # Process :context: {parent-of-context-<SHA>} line\n                            if line.startswith(':context: {parent-of-context-'):\n                                parsing_state = REGULAR_LINES\n                                new_file.write(':context: {parent-of-context-' + title_id_sha + '}\\n')\n                                continue\n                            else:\n                                print 'ERROR: Expected context restore line'\n                                sys.exit()\n            if parsing_state == LEGACY_MODE:\n                # Leave legacy files unchanged!\n                print '  - legacy file detected - no changes made'\n            else:\n                # Remove original file\n                os.remove(fixfile)\n                # Move new file\n                shutil.move(abs_path, fixfile)\n\n\n    def _generate_hash(self, text):\n        # Generates a 6-character hex encoded hash\n        hash = hashlib.sha256(text.encode('UTF-8')).hexdigest()\n        truncated_hash = hash[:6]\n        return truncated_hash\n\n\n    def toc(self, args):\n        pass\n\n\n    def atom(self, args):\n        head, tail = os.path.split(args.FILE)\n        type = self.type_of_file(tail)\n        if type not in ['assembly', 'procedure', 'concept', 'reference']:\n            print 'ERROR: File must be a module or an assembly: ' + args.FILE\n            sys.exit()\n        if type in ['procedure', 'concept', 'reference']:\n            type = 'module'\n        if (not args.parent) and (not args.siblings) and (not args.children):\n            # Set the default options\n            if type == 'assembly':\n                edit_parent = False\n                edit_siblings = False\n                edit_children = True\n            else: # type == 'module'\n                edit_parent = True\n                edit_siblings = True\n                edit_children = False\n        else:\n            edit_parent = args.parent\n            edit_siblings = args.siblings\n            edit_children = args.children\n        categoryset = self.scan_for_categories(self.context.ASSEMBLIES_DIR)\n        assemblyfiles = self.scan_for_categorised_files(self.context.ASSEMBLIES_DIR, categoryset, filefilter='assembly')\n        assemblyfiles.extend(self._scan_for_bookfiles())\n        parentassemblies, assemblyincludes = self._scan_for_parent_assemblies(assemblyfiles)\n        # Assemble the list of files to edit\n        targetfilelist = []\n        if edit_parent or edit_siblings:\n            if args.FILE in parentassemblies:\n                for parentassembly in parentassemblies[args.FILE]:\n                    if edit_parent:\n                        targetfilelist.append(parentassembly)\n                    if edit_siblings and (parentassembly in assemblyincludes):\n                        targetfilelist.extend(assemblyincludes[parentassembly])\n            else:\n                print 'WARN: Could not find parent assembly'\n        if not edit_siblings:\n            targetfilelist.append(args.FILE)\n        if edit_children and type == 'assembly':\n            if args.FILE in assemblyincludes:\n                targetfilelist.extend(assemblyincludes[args.FILE])\n        subprocess.check_call(['atom'] + targetfilelist)\n\n\n    def version(self, args):\n        pass\n\n\ndef add_module_arguments(parser):\n    parser.add_argument('CATEGORY', help='Category in which to store this module. Can use / as a separator to define sub-categories')\n    parser.add_argument('MODULE_ID', help='Unique ID to identify this module')\n    parser.add_argument('-u', '--user-story', help='Text of a user story (enclose in quotes)')\n    parser.add_argument('-t', '--title', help='Title of the module (enclose in quotes)')\n    parser.add_argument('-j', '--jira', help='Reference to a Jira issue related to the creation of this module')\n    parser.add_argument('-p', '--parent-assemblies', help='List of assemblies that include this module, specified as a space-separated list (enclose in quotes)')\n\n\n# MAIN CODE - PROGRAM STARTS HERE!\n# --------------------------------\n\n# Basic initialization\nif not os.path.exists('nebel.cfg'):\n  print 'WARN: No nebel.cfg file found in this directory.'\n  sys.exit()\ncontext = nebel.context.NebelContext()\ncontext.initializeFromFile('nebel.cfg')\nthis_script_path = os.path.dirname(os.path.abspath(__file__))\ncontext.templatePath = os.path.abspath(os.path.join(this_script_path, '..', 'template'))\ncontext.moduleFactory = nebel.factory.ModuleFactory(context)\ntasks = Tasks(context)\n\n# Create the top-level parser\nparser = argparse.ArgumentParser(prog='nebel')\nparser.add_argument('-v', '--version', action='version', version='Nebel 2.1.x (dev release)')\nsubparsers = parser.add_subparsers()\n\n# Create the sub-parser for the 'assembly' command\nassembly_parser = subparsers.add_parser('assembly', help='Generate an assembly')\nadd_module_arguments(assembly_parser)\nassembly_parser.set_defaults(func=tasks.create_assembly)\n\n# Create the sub-parser for the 'procedure' command\nprocedure_parser = subparsers.add_parser('procedure', help='Generate a procedure module')\nadd_module_arguments(procedure_parser)\nprocedure_parser.set_defaults(func=tasks.create_procedure)\n\n# Create the sub-parser for the 'concept' command\nconcept_parser = subparsers.add_parser('concept', help='Generate a concept module')\nadd_module_arguments(concept_parser)\nconcept_parser.set_defaults(func=tasks.create_concept)\n\n# Create the sub-parser for the 'reference' command\nreference_parser = subparsers.add_parser('reference', help='Generate a reference module')\nadd_module_arguments(reference_parser)\nreference_parser.set_defaults(func=tasks.create_reference)\n\n# Create the sub-parser for the 'create-from' command\ncreate_parser = subparsers.add_parser('create-from', help='Create multiple assemblies/modules from a CSV file, or an assembly file')\ncreate_parser.add_argument('FROM_FILE', help='Can be either a comma-separated values (CSV) file (ending with .csv), or an assembly file (starting with {}/ and ending with .adoc)'.format(context.ASSEMBLIES_DIR))\ncreate_parser.set_defaults(func=tasks.create_from)\n\n# Create the sub-parser for the 'split' command\nsplit_parser = subparsers.add_parser('split', help='Split an annotated AsciiDoc file into multiple assemblies and modules')\nsplit_parser.add_argument('FROM_FILE', help='Annotated AsciiDoc file (ending with .adoc, including optional wildcard braces, {})')\nsplit_parser.add_argument('--legacybasedir', help='Base directory for annotated file content. Subdirectories of this directory are used as default categories.')\nsplit_parser.add_argument('--category-prefix', help='When splitting an annotated file, add this prefix to default categories.')\nsplit_parser.add_argument('-a', '--attribute-files', help='Specify a comma-separated list of attribute files')\nsplit_parser.add_argument('--conditions', help='Define a comma-separated list of condition attributes, for resolving ifdef and ifndef directives')\nsplit_parser.add_argument('--timestamp', help='Generate a timestamp in the generated module and assembly files', action='store_true')\nsplit_parser.set_defaults(func=tasks.adoc_split)\n\n# Create the sub-parser for the 'book' command\nbook_parser = subparsers.add_parser('book', help='Create and manage book directories')\nbook_parser.add_argument('BOOK_DIR', help='The book directory')\nbook_parser.add_argument('--create', help='Create a new book directory', action='store_true')\nbook_parser.add_argument('-c', '--category-list', help='Comma-separated list of categories to add to book (enclose in quotes)')\nbook_parser.set_defaults(func=tasks.book)\n\n# Create the sub-parser for the 'mv' command\nbook_parser = subparsers.add_parser('mv', help='Move (or rename) module or assembly files. You can optionally use a single instance of braces for globbing/substituting. For example, to change a file prefix from p_ to proc_ you could enter: nebel mv p_{}.adoc proc_{}.adoc')\nbook_parser.add_argument('FROM_FILE', help='File origin. Optionally use {} for globbing.')\nbook_parser.add_argument('TO_FILE', help='File destination. Optionally use {} to substitute captured glob content')\nbook_parser.set_defaults(func=tasks.mv)\n\n# Create the sub-parser for the 'update' command\nupdate_parser = subparsers.add_parser('update', help='Update metadata in modules and assemblies')\nupdate_parser.add_argument('--fix-includes', help='Fix erroneous include directives in assemblies', action='store_true')\nupdate_parser.add_argument('--fix-links', help='Fix erroneous cross-reference links', action='store_true')\nupdate_parser.add_argument('-p','--parent-assemblies', help='Update ParentAssemblies property in modules and assemblies', action='store_true')\nupdate_parser.add_argument('--generate-ids', help='Generate missing IDs for headings', action='store_true')\nupdate_parser.add_argument('--add-contexts', help='Add _{context} to IDs and add boilerplate around include directives', action='store_true')\nupdate_parser.add_argument('--hash-contexts', help='Use together with --add-contexts if you want contexts to contain hashes instead of literal IDs', action='store_true')\nupdate_parser.add_argument('-c', '--category-list', help='Apply update only to this comma-separated list of categories (enclose in quotes)')\nupdate_parser.add_argument('-b', '--book', help='Apply update only to the specified book')\nupdate_parser.add_argument('-a', '--attribute-files', help='Specify a comma-separated list of attribute files')\nupdate_parser.add_argument('FILE', help='File to update OR you can omit this argument and use --book or --category-list instead', nargs='?')\nupdate_parser.set_defaults(func=tasks.update)\n\n# Create the sub-parser for the 'orphan' command\norphan_parser = subparsers.add_parser('orphan', help='Search for orphaned module and assembly files')\norphan_parser.add_argument('-c', '--category-list', help='Filter for orphan files belonging to this comma-separated list of categories')\norphan_parser.add_argument('-a', '--attribute-files', help='Specify a comma-separated list of attribute files')\norphan_parser.set_defaults(func=tasks.orphan_search)\n\n# Create the sub-parser for the 'toc' command\ntoc_parser = subparsers.add_parser('toc', help='List TOC for assembly or book')\ntoc_parser.add_argument('ASSEMBLY_OR_BOOK_FILE', help='Path to the assembly or book file whose table of contents you want to list')\ntoc_parser.set_defaults(func=tasks.toc)\n\n# Create the sub-parser for the 'atom' command\natom_parser = subparsers.add_parser('atom', help='Open a module or an assembly using the atom editor')\natom_parser.add_argument('FILE', help='Pathname of the assembly or module file to edit')\natom_parser.add_argument('-p', '--parent', help='Open the parent assembly of the specified assembly or module', action='store_true')\natom_parser.add_argument('-s', '--siblings', help='Open the siblings of the specified assembly or module', action='store_true')\natom_parser.add_argument('-c', '--children', help='Open the children of the specified assembly', action='store_true')\natom_parser.set_defaults(func=tasks.atom)\n\n\n# Now, parse the args and call the relevant sub-command\nargs = parser.parse_args()\nargs.func(args)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/nebel/commands.py b/nebel/commands.py
--- a/nebel/commands.py	(revision 1bc0b02d66cdf9081a5604adef47921f6e7eb456)
+++ b/nebel/commands.py	(date 1654624890668)
@@ -4,18 +4,20 @@
 @author fbolton
 '''
 
+import argparse
+import datetime
+import glob
+import hashlib
 import os
 import re
+import shutil
+import subprocess
 import sys
 import tempfile
-import shutil
-import argparse
+
 import nebel.context
 import nebel.factory
-import datetime
-import glob
-import hashlib
-import subprocess
+
 
 class Tasks:
     def __init__(self, context):
@@ -37,25 +39,26 @@
             for assemblyfile in args.parent_assemblies.split():
                 self.add_include_to_assembly(assemblyfile, modulefile)
 
-    def create_assembly(self,args):
-        metadata = {'Type':'assembly'}
+    def create_assembly(self, args):
+        metadata = {'Type': 'assembly'}
         self._create(args, metadata)
 
-    def create_procedure(self,args):
-        metadata = {'Type':'procedure'}
+    def create_procedure(self, args):
+        metadata = {'Type': 'procedure'}
         self._create(args, metadata)
 
-    def create_concept(self,args):
-        metadata = {'Type':'concept'}
+    def create_concept(self, args):
+        metadata = {'Type': 'concept'}
         self._create(args, metadata)
 
-    def create_reference(self,args):
-        metadata = {'Type':'reference'}
+    def create_reference(self, args):
+        metadata = {'Type': 'reference'}
         self._create(args, metadata)
 
     def add_include_to_assembly(self, assemblyfile, includedfile, leveloffset=1):
         if not os.path.exists(assemblyfile):
-            print 'WARN: Referenced assembly file does not exist:' + assemblyfile
+            print
+            'WARN: Referenced assembly file does not exist:' + assemblyfile
             return
         # Create temp file
         fh, abs_path = tempfile.mkstemp()
@@ -87,19 +90,19 @@
         # Move new file
         shutil.move(abs_path, assemblyfile)
 
-
-    def create_from(self,args):
+    def create_from(self, args):
         fromfile = args.FROM_FILE
         if fromfile.endswith('.csv'):
             self._create_from_csv(args)
             return
-        elif fromfile.startswith(self.context.ASSEMBLIES_DIR)\
-                and fromfile.endswith('.adoc')\
+        elif fromfile.startswith(self.context.ASSEMBLIES_DIR) \
+                and fromfile.endswith('.adoc') \
                 and os.path.basename(fromfile).startswith(self.context.ASSEMBLY_PREFIX):
             self._create_from_assembly(args)
             return
         else:
-            print 'ERROR: Unknown file type [' + fromfile + ']: must end either in .csv or .adoc'
+            print
+            'ERROR: Unknown file type [' + fromfile + ']: must end either in .csv or .adoc'
             sys.exit()
 
     def type_of_file(self, basename):
@@ -136,7 +139,7 @@
         title = re.sub(r'\s+', '-', title)
         return title
 
-    def _create_from_assembly(self,args):
+    def _create_from_assembly(self, args):
         asfile = args.FROM_FILE
         regexp = re.compile(r'^\s*include::[\./]*modules/([^\[]+)\[[^\]]*\]')
         with open(asfile, 'r') as f:
@@ -147,7 +150,8 @@
                     category, basename = os.path.split(modulefile)
                     type = self.type_of_file(basename)
                     if type is not None and basename.endswith('.adoc'):
-                        print modulefile
+                        print
+                        modulefile
                         metadata = {}
                         metadata['Type'] = type
                         metadata['Category'] = category
@@ -155,7 +159,6 @@
                         metadata['ParentAssemblies'] = asfile
                         self.context.moduleFactory.create(metadata)
 
-
     def adoc_split(self, args):
         frompattern = os.path.normpath(args.FROM_FILE)
         fromfiles = glob.glob(frompattern.replace('{}', '*'))
@@ -173,7 +176,8 @@
             categoryname = 'default'
             if args.legacybasedir:
                 if not os.path.exists(args.legacybasedir):
-                    print 'ERROR: No such base directory: ' + args.legacybasedir
+                    print
+                    'ERROR: No such base directory: ' + args.legacybasedir
                     sys.exit()
                 relativedir = os.path.dirname(os.path.relpath(fromfile, args.legacybasedir))
                 categoryname = relativedir.replace(os.path.sep, '-')
@@ -183,7 +187,8 @@
             equalssigncount = 0
             lines = self._resolve_includes(fromfile)
             indexofnextline = 0
-            self._parse_from_annotated(metadata, fromfile, lines, indexofnextline, equalssigncount, selectedconditions, args.timestamp)
+            self._parse_from_annotated(metadata, fromfile, lines, indexofnextline, equalssigncount, selectedconditions,
+                                       args.timestamp)
 
     def _parse_from_annotated(
             self,
@@ -192,10 +197,10 @@
             lines,
             indexofnextline,
             equalssigncount,
-            selectedconditions = None,
-            timestamp = False,
-            showcontentstack = [],
-            currconditionstack = []
+            selectedconditions=None,
+            timestamp=False,
+            showcontentstack=[],
+            currconditionstack=[]
     ):
         # Define some enums for state machine
         REGULAR_LINES = 0
@@ -222,12 +227,12 @@
         regexp_metadata = re.compile(r'^\s*//\s*(\w+)\s*:\s*(.*)')
         regexp_id_line1 = re.compile(r'^\s*\[\[\s*(\S+)\s*\]\]\s*$')
         regexp_id_line2 = re.compile(r'^\s*\[id\s*=\s*[\'"]\s*(\S+)\s*[\'"]\]\s*$')
-        regexp_ifdef    = re.compile(r'^ifdef::([^\[]+)\[\]')
+        regexp_ifdef = re.compile(r'^ifdef::([^\[]+)\[\]')
         regexp_ifdef_single = re.compile(r'^ifdef::([^\[]+)\[([^\]]+)\]')
-        regexp_ifndef   = re.compile(r'^ifndef::([^\[]+)\[\]')
+        regexp_ifndef = re.compile(r'^ifndef::([^\[]+)\[\]')
         regexp_ifndef_single = re.compile(r'^ifndef::([^\[]+)\[([^\]]+)\]')
-        regexp_ifeval   = re.compile(r'^ifeval::\[([^\]]*)\]')
-        regexp_endif    = re.compile(r'^endif::([^\[]*)\[\]')
+        regexp_ifeval = re.compile(r'^ifeval::\[([^\]]*)\]')
+        regexp_endif = re.compile(r'^endif::([^\[]*)\[\]')
         regexp_title = re.compile(r'^(=+)\s+(\S.*)')
 
         childmetadata = {}
@@ -293,7 +298,7 @@
                 if result is not None:
                     currconditionstack.append('')
                     showcontentstack.append(showcontent)
-                    print ('WARNING: ifeval not supported: defaults to showing content')
+                    print('WARNING: ifeval not supported: defaults to showing content')
                     # Do not include tagged line in output
                     indexofnextline += 1
                     continue
@@ -303,7 +308,7 @@
                     matchcondition = currconditionstack.pop()
                     showcontent = showcontentstack.pop()
                     if (conditionname) and (conditionname != matchcondition):
-                        print ('WARNING: Unmatched condition tags: ' + conditionname + '!=' + matchcondition)
+                        print('WARNING: Unmatched condition tags: ' + conditionname + '!=' + matchcondition)
                     # Do not include tagged line in output
                     indexofnextline += 1
                     continue
@@ -314,7 +319,8 @@
 
             if parsing_state == REGULAR_LINES:
                 line = lines[indexofnextline]
-                if (regexp_metadata.search(line) is None) and (regexp_id_line1.search(line) is None) and (regexp_id_line2.search(line) is None) and (regexp_title.search(line) is None):
+                if (regexp_metadata.search(line) is None) and (regexp_id_line1.search(line) is None) and (
+                        regexp_id_line2.search(line) is None) and (regexp_title.search(line) is None):
                     # Regular line
                     parsedcontentlines.append(line)
                     indexofnextline += 1
@@ -369,7 +375,8 @@
                         parsedcontentlines.append(lastline)
                     elif action == CREATE_MODULE_OR_ASSEMBLY:
                         if ('ModuleID' not in childmetadata):
-                            print 'ERROR: Heading ' + title + ' must have a module ID.'
+                            print
+                            'ERROR: Heading ' + title + ' must have a module ID.'
                             sys.exit()
                         if ('Category' not in childmetadata):
                             childmetadata['Category'] = metadata['Category']
@@ -387,7 +394,7 @@
                             showcontentstack,
                             currconditionstack
                         )
-                        #if ('Type' in childmetadata) and (childmetadata['Type'].lower() == 'assembly'):
+                        # if ('Type' in childmetadata) and (childmetadata['Type'].lower() == 'assembly'):
                         #    print ('include::' + generated_file + '[leveloffset=+1]')
                         childmetadata = {}
                         parsedcontentlines.append('\n')
@@ -416,8 +423,9 @@
                     if metadata_name in self.context.allMetadataFields:
                         childmetadata[metadata_name] = metadata_value
                     else:
-                        print 'WARNING: Unknown metadata "' + metadata_name + '" in file ' + fromfilepath
-                    #print 'Metadata: ' + metadata_name + ' = ' + metadata_value
+                        print
+                        'WARNING: Unknown metadata "' + metadata_name + '" in file ' + fromfilepath
+                    # print 'Metadata: ' + metadata_name + ' = ' + metadata_value
                     continue
                 # Parse ID line
                 original_id = ''
@@ -446,8 +454,6 @@
                     expecting_title_line = False
                     childmetadata = {}
 
-
-
     def _scan_file_for_includes(self, asfile, recursive=False):
         includedfilelist = []
         regexp = re.compile(r'^\s*include::([^\[]+)\[[^\]]*\]')
@@ -457,14 +463,16 @@
                 if result is not None:
                     includedfile = self.context.resolve_raw_attribute_value(result.group(1))
                     directory = os.path.dirname(asfile)
-                    path_to_included_file = os.path.relpath(os.path.realpath(os.path.normpath(os.path.join(directory, includedfile))))
+                    path_to_included_file = os.path.relpath(
+                        os.path.realpath(os.path.normpath(os.path.join(directory, includedfile))))
                     if includedfile.endswith('.adoc'):
                         includedfilelist.append(path_to_included_file)
         allincludedfilelist = includedfilelist
         if (recursive):
             for file in includedfilelist:
                 if not os.path.exists(file):
-                    print 'ERROR: While scanning ' + asfile + ': included file, ' + file + ', does not exist'
+                    print
+                    'ERROR: While scanning ' + asfile + ': included file, ' + file + ', does not exist'
                     sys.exit()
                 childincludedfilelist = self._scan_file_for_includes(file, recursive=True)
                 allincludedfilelist.extend(childincludedfilelist)
@@ -473,7 +481,8 @@
     def _resolve_includes(self, file, baselevel=0, selectedtags=None):
         # Resolve all of the nested includes in 'file' to plain text and return a plain text array of all the lines in the file
         if not os.path.exists(file):
-            print 'ERROR: Include file not found: ' + file
+            print
+            'ERROR: Include file not found: ' + file
             sys.exit()
         if (selectedtags is not None) and (len(selectedtags) > 0):
             istaggingactive = True
@@ -487,7 +496,7 @@
         regexp_include = re.compile(r'^\s*include::([^\[]+)\[([^\]]*)\]')
         regexp_title = re.compile(r'^(=+)\s+(\S.*)')
         regexp_tag_begin = re.compile(r'tag::([^\[]+)\[\]')
-        regexp_tag_end   = re.compile(r'end::([^\[]+)\[\]')
+        regexp_tag_end = re.compile(r'end::([^\[]+)\[\]')
         with open(file, 'r') as f:
             for line in f:
                 if istaggingactive:
@@ -520,7 +529,7 @@
                 result = regexp_include.search(line)
                 if result is not None:
                     includedfile = self.context.resolve_raw_attribute_value(result.group(1))
-                    options      = result.group(2)
+                    options = result.group(2)
                     optmap = self._parse_include_opts(options)
                     childbaselevel = baselevel
                     if 'leveloffset' in optmap:
@@ -530,13 +539,15 @@
                         else:
                             childbaselevel = int(leveloffset)
                     directory = os.path.dirname(file)
-                    path_to_included_file = os.path.relpath(os.path.realpath(os.path.normpath(os.path.join(directory, includedfile))))
+                    path_to_included_file = os.path.relpath(
+                        os.path.realpath(os.path.normpath(os.path.join(directory, includedfile))))
                     taglist = []
                     if ('tag' in optmap):
                         taglist.append(optmap['tag'].strip())
                     if ('tags' in optmap):
                         taglist.extend(optmap['tags'].split(';'))
-                    linesinfile.extend(self._resolve_includes(path_to_included_file, baselevel=childbaselevel, selectedtags=taglist))
+                    linesinfile.extend(
+                        self._resolve_includes(path_to_included_file, baselevel=childbaselevel, selectedtags=taglist))
                     continue
                 linesinfile.append(line)
         return linesinfile
@@ -551,12 +562,12 @@
                 optmap[prop.strip().lower()] = value.strip()
         return optmap
 
-    def _create_from_csv(self,args):
+    def _create_from_csv(self, args):
         csvfile = args.FROM_FILE
         USING_LEVELS = False
         with open(csvfile, 'r') as filehandle:
             # First line should be the column headings
-            headings = filehandle.readline().strip().replace(' ','')
+            headings = filehandle.readline().strip().replace(' ', '')
             headinglist = headings.split(',')
             # Alias 'NestingLevel' to 'Level'
             if 'NestingLevel' in headinglist:
@@ -568,7 +579,8 @@
                 headinglist[k] = 'ModuleID'
             # Check plausibility of headinglist
             if ('Category' not in headinglist) or ('ModuleID' not in headinglist):
-                print 'ERROR: CSV file does not have correct format'
+                print
+                'ERROR: CSV file does not have correct format'
                 sys.exit()
             if 'Level' in headinglist:
                 USING_LEVELS = True
@@ -590,12 +602,13 @@
                     metadata = dict(zip(headinglist, fieldlist))
                     # Skip rows with Implement field set to 'no'
                     if ('Implement' in metadata) and (metadata['Implement'].lower() == 'no'):
-                        print 'INFO: Skipping unimplemented module/assembly: ' + metadata['ModuleID']
+                        print
+                        'INFO: Skipping unimplemented module/assembly: ' + metadata['ModuleID']
                         continue
                     # Weed out irrelevant metadata entries
-                    for field,value in metadata.items():
+                    for field, value in metadata.items():
                         if field not in self.context.allMetadataFields:
-                            del(metadata[field])
+                            del (metadata[field])
                     if metadata['Type'] == '':
                         # Assume it's an empty row (i.e. fields are empty, row is just commas)
                         if (not USING_LEVELS) and (currentlevel == 1):
@@ -628,7 +641,6 @@
                         currentfile = newfile
                         currentlevel = level
 
-
     def smart_split(self, line, splitchar=',', preserveQuotes=False):
         list = []
         isInQuotes = False
@@ -658,10 +670,10 @@
             list.append(currfield)
         return list
 
-
-    def book(self,args):
+    def book(self, args):
         if self.context.ASSEMBLIES_DIR == '.' or self.context.MODULES_DIR == '.':
-            print 'ERROR: book command is only usable for a standard directory layout, with defined assemblies and modules directories'
+            print
+            'ERROR: book command is only usable for a standard directory layout, with defined assemblies and modules directories'
             sys.exit()
         if args.create:
             # Create book and (optionally) add categories
@@ -670,13 +682,14 @@
             # Add categories
             self._book_categories(args)
         else:
-            print 'ERROR: No options specified'
-
+            print
+            'ERROR: No options specified'
 
-    def _book_create(self,args):
+    def _book_create(self, args):
         bookdir = args.BOOK_DIR
         if os.path.exists(bookdir):
-            print 'ERROR: Book directory already exists: ' + bookdir
+            print
+            'ERROR: Book directory already exists: ' + bookdir
             sys.exit()
         os.mkdir(bookdir)
         os.mkdir(os.path.join(bookdir, self.context.ASSEMBLIES_DIR))
@@ -695,11 +708,11 @@
         if args.category_list:
             self._book_categories(args)
 
-
     def _book_categories(self, args):
         bookdir = args.BOOK_DIR
         if not os.path.exists(bookdir):
-            print 'ERROR: Book directory does not exist: ' + bookdir
+            print
+            'ERROR: Book directory does not exist: ' + bookdir
             sys.exit()
         imagesdir = os.path.join(bookdir, self.context.IMAGES_DIR)
         modulesdir = os.path.join(bookdir, self.context.MODULES_DIR)
@@ -729,10 +742,11 @@
                     os.path.join(assembliesdir, category)
                 )
 
-
-    def update(self,args):
-        if (not args.fix_includes) and (not args.parent_assemblies) and (not args.fix_links) and (not args.generate_ids) and (not args.add_contexts):
-            print 'ERROR: Missing required option(s)'
+    def update(self, args):
+        if (not args.fix_includes) and (not args.parent_assemblies) and (not args.fix_links) and (
+        not args.generate_ids) and (not args.add_contexts):
+            print
+            'ERROR: Missing required option(s)'
             sys.exit()
         if args.attribute_files:
             attrfilelist = args.attribute_files.strip().split(',')
@@ -742,13 +756,14 @@
             head, tail = os.path.split(args.FILE)
             type = self.type_of_file(tail)
             if type == 'assembly':
-                assemblyfiles = [ args.FILE ]
+                assemblyfiles = [args.FILE]
                 modulefiles = []
             elif type in ['procedure', 'concept', 'reference']:
                 assemblyfiles = []
                 modulefiles = [args.FILE]
             else:
-                print 'ERROR: File must be a module or an assembly: ' + args.FILE
+                print
+                'ERROR: File must be a module or an assembly: ' + args.FILE
                 sys.exit()
         else:
             # Determine the set of categories to update
@@ -758,13 +773,16 @@
                 map(str.strip, categoryset)
             elif args.book:
                 if not os.path.exists(args.book):
-                    print 'ERROR: ' + args.book + ' directory does not exist.'
+                    print
+                    'ERROR: ' + args.book + ' directory does not exist.'
                     sys.exit()
-                categoryset = self.scan_for_categories(os.path.join(args.book, self.context.MODULES_DIR))\
+                categoryset = self.scan_for_categories(os.path.join(args.book, self.context.MODULES_DIR)) \
                               | self.scan_for_categories(os.path.join(args.book, self.context.ASSEMBLIES_DIR))
             else:
-                categoryset = self.scan_for_categories(self.context.MODULES_DIR) | self.scan_for_categories(self.context.ASSEMBLIES_DIR)
-            assemblyfiles = self.scan_for_categorised_files(self.context.ASSEMBLIES_DIR, categoryset, filefilter='assembly')
+                categoryset = self.scan_for_categories(self.context.MODULES_DIR) | self.scan_for_categories(
+                    self.context.ASSEMBLIES_DIR)
+            assemblyfiles = self.scan_for_categorised_files(self.context.ASSEMBLIES_DIR, categoryset,
+                                                            filefilter='assembly')
             modulefiles = self.scan_for_categorised_files(self.context.MODULES_DIR, categoryset, filefilter='module')
             imagefiles = self.scan_for_categorised_files(self.context.IMAGES_DIR, categoryset)
         # Select the kind of update to implement
@@ -779,7 +797,6 @@
         if args.add_contexts:
             self._add_contexts(assemblyfiles, modulefiles, attrfilelist, args)
 
-
     def scan_for_categories(self, rootdir):
         categoryset = set()
         cwd = os.getcwd()
@@ -792,7 +809,6 @@
         categoryset.add('')
         return categoryset
 
-
     def scan_for_categorised_files(self, rootdir, categoryset, filefilter=None):
         filelist = []
         for category in categoryset:
@@ -805,11 +821,11 @@
                             filelist.append(pathname)
                         elif filefilter == 'assembly' and self.type_of_file(entry) == 'assembly':
                             filelist.append(pathname)
-                        elif filefilter == 'module' and self.type_of_file(entry) in ['module', 'concept', 'procedure', 'reference']:
+                        elif filefilter == 'module' and self.type_of_file(entry) in ['module', 'concept', 'procedure',
+                                                                                     'reference']:
                             filelist.append(pathname)
         return filelist
 
-
     def _update_fix_includes(self, assemblyfiles, modulefiles):
         # Create dictionaries mapping norm(filename) -> [pathname, pathname, ...]
         assemblyfiledict = {}
@@ -832,9 +848,9 @@
         for assemblyfile in assemblyfiles:
             self._update_include_directives(assemblyfile, assemblyfiledict, modulefiledict)
 
-
     def _update_include_directives(self, file, assemblyfiledict, modulefiledict):
-        print 'Updating include directives for file: ' + file
+        print
+        'Updating include directives for file: ' + file
         regexp = re.compile(r'^\s*include::([^\[\{]+)\[([^\]]*)\]')
         dirname = os.path.dirname(file)
         # Create temp file
@@ -843,7 +859,7 @@
             with open(file) as old_file:
                 for line in old_file:
                     if line.lstrip().startswith('include::'):
-                        #print '\t' + line.strip()
+                        # print '\t' + line.strip()
                         result = regexp.search(line)
                         if result is not None:
                             includepath = result.group(1)
@@ -857,8 +873,10 @@
                                         pathlist = assemblyfiledict[normincludefile]
                                         new_includepath = self.choose_includepath(dirname, pathlist)
                                         if new_includepath is not None:
-                                            new_file.write('include::' + new_includepath + '[' + result.group(2) + ']\n')
-                                            print 'Replacing: ' + includepath + ' with ' + new_includepath
+                                            new_file.write(
+                                                'include::' + new_includepath + '[' + result.group(2) + ']\n')
+                                            print
+                                            'Replacing: ' + includepath + ' with ' + new_includepath
                                             continue
                                 else:
                                     # Module case
@@ -866,26 +884,31 @@
                                         pathlist = modulefiledict[normincludefile]
                                         new_includepath = self.choose_includepath(dirname, pathlist)
                                         if new_includepath is not None:
-                                            new_file.write('include::' + new_includepath + '[' + result.group(2) + ']\n')
-                                            print 'Replacing: ' + includepath + ' with ' + new_includepath
+                                            new_file.write(
+                                                'include::' + new_includepath + '[' + result.group(2) + ']\n')
+                                            print
+                                            'Replacing: ' + includepath + ' with ' + new_includepath
                                             continue
                         else:
-                            print 'WARN: Unparsable include:' + line.strip()
+                            print
+                            'WARN: Unparsable include:' + line.strip()
                     new_file.write(line)
         # Remove original file
         os.remove(file)
         # Move new file
         shutil.move(abs_path, file)
 
-
     def choose_includepath(self, basedir, pathlist):
         if len(pathlist) == 1:
             return os.path.relpath(pathlist[0], basedir)
         else:
-            print '\tChoose the correct path for the included file or S to skip:'
+            print
+            '\tChoose the correct path for the included file or S to skip:'
             for k, path in enumerate(pathlist):
-                print '\t' + str(k) + ') ' + path
-            print '\tS) Skip and leave this include unchanged'
+                print
+                '\t' + str(k) + ') ' + path
+            print
+            '\tS) Skip and leave this include unchanged'
             response = ''
             while response.strip() == '':
                 response = raw_input('\tEnter selection [S]: ')
@@ -899,7 +922,6 @@
                     response = ''
             return None
 
-
     def _scan_for_parent_assemblies(self, assemblylist):
         # Create dictionary of modules included by assemblies
         assemblyincludes = {}
@@ -916,7 +938,6 @@
                     parentassemblies[modulefile].append(assemblyfile)
         return parentassemblies, assemblyincludes
 
-
     def _update_parent_assemblies(self, assemblylist):
         parentassemblies, assemblyincludes = self._scan_for_parent_assemblies(assemblylist)
         # Update the ParentAssemblies metadata in each of the module files
@@ -937,7 +958,7 @@
                     booklist.append(bookfile)
         return booklist
 
-    def _update_fix_links(self, assemblyfiles, modulefiles, attrfilelist = None):
+    def _update_fix_links(self, assemblyfiles, modulefiles, attrfilelist=None):
         # Set of files whose links should be fixed
         fixfileset = set(assemblyfiles) | set(modulefiles)
         # Identify top-level book files to scan
@@ -950,12 +971,15 @@
         for bookfile in booklist:
             booktitle = self._scan_for_title(bookfile)
             booktitle_slug = self._convert_title_to_slug(booktitle)
-            #print 'Title URL slug: ' + booktitle_slug
-            print 'Title: ' + booktitle
+            # print 'Title URL slug: ' + booktitle_slug
+            print
+            'Title: ' + booktitle
             self.context.clear_attributes()
-            anchorid_dict, legacyid_dict, rootofid_dict = self._parse_file_for_anchorids(anchorid_dict, legacyid_dict, rootofid_dict, booktitle_slug, bookfile)
-            #print anchorid_dict.keys()
-        #print anchorid_dict
+            anchorid_dict, legacyid_dict, rootofid_dict = self._parse_file_for_anchorids(anchorid_dict, legacyid_dict,
+                                                                                         rootofid_dict, booktitle_slug,
+                                                                                         bookfile)
+            # print anchorid_dict.keys()
+        # print anchorid_dict
         self.anchorid_dict = anchorid_dict
         self.legacyid_dict = legacyid_dict
         self.rootofid_dict = rootofid_dict
@@ -968,7 +992,8 @@
         self.parentassemblies = parentassemblies
 
         for fixfile in fixfileset:
-            print 'Updating links for file: ' + fixfile
+            print
+            'Updating links for file: ' + fixfile
             dirname = os.path.dirname(fixfile)
             # Create temp file
             fh, abs_path = tempfile.mkstemp()
@@ -986,19 +1011,16 @@
             # Move new file
             shutil.move(abs_path, fixfile)
 
-
     def _regexp_replace_angles(self, value):
         regexp = re.compile(r'<<([^,>]+),?([^>]*)>>')
         new_value = regexp.sub(self._on_match_xref, value)
         return new_value
 
-
     def _regexp_replace_xref(self, value):
         regexp = re.compile(r'xref:([\w\-]+)\[([^\]]*)\]')
         new_value = regexp.sub(self._on_match_xref, value)
         return new_value
 
-
     def _on_match_xref(self, match_obj):
         anchorid = match_obj.group(1)
         optionaltext = match_obj.group(2)
@@ -1025,7 +1047,7 @@
 
     def _repair_anchorid(self, anchorid, fixfile):
         if anchorid.endswith('_{context}'):
-            plainanchorid = anchorid.replace('_{context}','')
+            plainanchorid = anchorid.replace('_{context}', '')
         else:
             plainanchorid = anchorid
         if plainanchorid in self.anchorid_dict:
@@ -1038,17 +1060,18 @@
                 # Leave the ID unchanged
                 target_anchorid = anchorid
         elif '_' in plainanchorid:
-                # Last attempt to fix - ID might have wrong context value after the '_' char
-                rootofid, contextval = plainanchorid.rsplit('_', 1)
-                if rootofid in self.rootofid_dict:
-                    target_anchorid = self.choose_anchorid_from_rootofid_dict(rootofid)
-                    if target_anchorid is None:
-                        # Leave the ID unchanged
-                        target_anchorid = anchorid
-                else:
-                    target_anchorid = anchorid
+            # Last attempt to fix - ID might have wrong context value after the '_' char
+            rootofid, contextval = plainanchorid.rsplit('_', 1)
+            if rootofid in self.rootofid_dict:
+                target_anchorid = self.choose_anchorid_from_rootofid_dict(rootofid)
+                if target_anchorid is None:
+                    # Leave the ID unchanged
+                    target_anchorid = anchorid
+            else:
+                target_anchorid = anchorid
         else:
-            print 'WARNING: link to unknown ID: ' + anchorid
+            print
+            'WARNING: link to unknown ID: ' + anchorid
             target_anchorid = anchorid
 
         # Special case: if the file containing the xref and the file containing the target ID have the *same* parent assembly,
@@ -1060,7 +1083,8 @@
                 if target_anchorid in self.anchorid_dict:
                     for booktitle_slug in self.anchorid_dict[target_anchorid]:
                         targetfile = self.anchorid_dict[target_anchorid][booktitle_slug]['FilePath']
-                        if (targetfile.startswith(self.context.MODULES_DIR)) and (parent in self.parentassemblies[targetfile]):
+                        if (targetfile.startswith(self.context.MODULES_DIR)) and (
+                                parent in self.parentassemblies[targetfile]):
                             use_context_suffix = True
             if use_context_suffix:
                 target_anchorid = rootofid + '_{context}'
@@ -1071,10 +1095,13 @@
         if len(idlist) == 1:
             return idlist[0]
         else:
-            print '\tChoose the correct target ID for the link or S to skip:'
+            print
+            '\tChoose the correct target ID for the link or S to skip:'
             for k, targetid in enumerate(idlist):
-                print '\t' + str(k) + ') ' + targetid
-            print '\tS) Skip and leave this include unchanged'
+                print
+                '\t' + str(k) + ') ' + targetid
+            print
+            '\tS) Skip and leave this include unchanged'
             response = ''
             while response.strip() == '':
                 response = raw_input('\tEnter selection [S]: ')
@@ -1090,7 +1117,8 @@
 
     def _scan_for_title(self, filepath):
         if not os.path.exists(filepath):
-            print 'ERROR: _scan_for_title: No such file: ' + filepath
+            print
+            'ERROR: _scan_for_title: No such file: ' + filepath
             sys.exit()
         rawtitle = ''
         regexp = re.compile(r'^=\s+(\S.*)')
@@ -1101,15 +1129,14 @@
                     rawtitle = result.group(1)
                     break
             if rawtitle == '':
-                print 'ERROR: _scan_for_title: No title found in file: ' + filepath
+                print
+                'ERROR: _scan_for_title: No title found in file: ' + filepath
                 sys.exit()
         return self.context.resolve_raw_attribute_value(rawtitle)
 
-
     def _convert_title_to_slug(self, title):
         return title.strip().lower().replace(' ', '_').replace('-', '_')
 
-
     def _parse_file_for_anchorids(self, anchorid_dict, legacyid_dict, rootofid_dict, booktitle_slug, filepath):
         # Define action enums
         NO_ACTION = 0
@@ -1131,7 +1158,8 @@
         regexp_blank = re.compile(r'^\s*$')
 
         if not os.path.exists(filepath):
-            print 'ERROR: _parse_file_for_anchorids: File does not exist: ' + filepath
+            print
+            'ERROR: _parse_file_for_anchorids: File does not exist: ' + filepath
             sys.exit()
         with open(filepath, 'r') as filehandle:
             tentative_metadata = {}
@@ -1191,9 +1219,11 @@
                         # Initialize the sub-dictionary
                         anchorid_dict[tentative_anchor_id] = {}
                     if booktitle_slug in anchorid_dict[tentative_anchor_id]:
-                        print 'WARNING: Anchor ID: ' + tentative_anchor_id + 'appears more than once in book: ' + booktitle_slug
+                        print
+                        'WARNING: Anchor ID: ' + tentative_anchor_id + 'appears more than once in book: ' + booktitle_slug
                     else:
-                        anchorid_dict[tentative_anchor_id][booktitle_slug] = { 'FilePath': os.path.relpath(os.path.realpath(filepath)) }
+                        anchorid_dict[tentative_anchor_id][booktitle_slug] = {
+                            'FilePath': os.path.relpath(os.path.realpath(filepath))}
                     tentative_anchor_id = ''
                     tentative_root_of_id = ''
                     tentative_context_of_id = None
@@ -1211,9 +1241,12 @@
                             anchorid = rawanchorid.replace('{context}', currentcontext)
                             rootofid = rawanchorid.replace('_{context}', '')
                         else:
-                            print 'ERROR: Found ID with embedded {context}, but no context attribute defined'
-                            print '    file: ' + filepath
-                            print '    ID:   ' + rawanchorid
+                            print
+                            'ERROR: Found ID with embedded {context}, but no context attribute defined'
+                            print
+                            '    file: ' + filepath
+                            print
+                            '    ID:   ' + rawanchorid
                             sys.exit()
                     else:
                         anchorid = rawanchorid
@@ -1228,16 +1261,20 @@
                         # Initialize the sub-dictionary
                         anchorid_dict[tentative_anchor_id] = {}
                     if booktitle_slug in anchorid_dict[tentative_anchor_id]:
-                        print 'WARNING: Anchor ID: ' + tentative_anchor_id + 'appears more than once in book: ' + booktitle_slug
+                        print
+                        'WARNING: Anchor ID: ' + tentative_anchor_id + 'appears more than once in book: ' + booktitle_slug
                     else:
-                        anchorid_dict[tentative_anchor_id][booktitle_slug] = { 'FilePath': os.path.relpath(os.path.realpath(filepath)), 'Title': title, 'Context': tentative_context_of_id }
+                        anchorid_dict[tentative_anchor_id][booktitle_slug] = {
+                            'FilePath': os.path.relpath(os.path.realpath(filepath)), 'Title': title,
+                            'Context': tentative_context_of_id}
                         if 'ConvertedFromID' in tentative_metadata:
-                            anchorid_dict[tentative_anchor_id][booktitle_slug]['ConvertedFromID'] = tentative_metadata['ConvertedFromID']
+                            anchorid_dict[tentative_anchor_id][booktitle_slug]['ConvertedFromID'] = tentative_metadata[
+                                'ConvertedFromID']
                             legacyid_dict[tentative_metadata['ConvertedFromID']] = tentative_anchor_id
                         if tentative_root_of_id != tentative_anchor_id:
                             if tentative_root_of_id not in rootofid_dict:
                                 # Initialize list of anchor IDs in this slot
-                                rootofid_dict[tentative_root_of_id] = [ tentative_anchor_id ]
+                                rootofid_dict[tentative_root_of_id] = [tentative_anchor_id]
                             else:
                                 rootofid_dict[tentative_root_of_id].append(tentative_anchor_id)
                     tentative_anchor_id = ''
@@ -1253,9 +1290,14 @@
                     currentdir, basename = os.path.split(filepath)
                     includefile = os.path.normpath(os.path.join(currentdir, includefile))
                     if not os.path.exists(includefile):
-                        print 'ERROR: Included file does not exist: ' + includefile
+                        print
+                        'ERROR: Included file does not exist: ' + includefile
                         sys.exit()
-                    anchorid_dict, legacyid_dict, rootofid_dict = self._parse_file_for_anchorids(anchorid_dict, legacyid_dict, rootofid_dict, booktitle_slug, includefile)
+                    anchorid_dict, legacyid_dict, rootofid_dict = self._parse_file_for_anchorids(anchorid_dict,
+                                                                                                 legacyid_dict,
+                                                                                                 rootofid_dict,
+                                                                                                 booktitle_slug,
+                                                                                                 includefile)
                     tentative_anchor_id = ''
                     tentative_root_of_id = ''
                     tentative_context_of_id = None
@@ -1272,7 +1314,8 @@
         regexp_title = re.compile(r'^(=+)\s+(\S.*)')
 
         for fixfile in fixfileset:
-            print 'Adding missing IDs to file: ' + fixfile
+            print
+            'Adding missing IDs to file: ' + fixfile
             dirname, basename = os.path.split(os.path.normpath(fixfile))
             idprefix = dirname.replace(os.sep, '-').replace('_', '-') + '-' + self.moduleid_of_file(basename)
             # Create temp file
@@ -1283,8 +1326,8 @@
                     newidlist = []
                     disambig_suffix = 1
                     for line in old_file:
-                        if (regexp_title.search(line) is not None)\
-                                and (regexp_id_line1.search(prevline) is None)\
+                        if (regexp_title.search(line) is not None) \
+                                and (regexp_id_line1.search(prevline) is None) \
                                 and (regexp_id_line2.search(prevline) is None):
                             # Parse title line
                             result = regexp_title.search(line)
@@ -1303,9 +1346,9 @@
             # Move new file
             shutil.move(abs_path, fixfile)
 
-
     def update_metadata(self, file, metadata):
-        print 'Updating metadata for file: ' + file
+        print
+        'Updating metadata for file: ' + file
         regexp = re.compile(r'^\s*//\s*(\w+)\s*:.*')
         # Scan file for pre-existing metadata settings
         preexisting = set()
@@ -1372,20 +1415,22 @@
             includedfiles = self._scan_file_for_includes(bookfile, recursive=True)
             allincludedfileset |= set(includedfiles)
         # Find the set of all known module and assemblyfiles
-        categoryset = self.scan_for_categories(self.context.MODULES_DIR) | self.scan_for_categories(self.context.ASSEMBLIES_DIR)
+        categoryset = self.scan_for_categories(self.context.MODULES_DIR) | self.scan_for_categories(
+            self.context.ASSEMBLIES_DIR)
         if filtercategoryset is not None:
             categoryset &= filtercategoryset
         assemblyfiles = self.scan_for_categorised_files(self.context.ASSEMBLIES_DIR, categoryset, filefilter='assembly')
         modulefiles = self.scan_for_categorised_files(self.context.MODULES_DIR, categoryset, filefilter='module')
-        #imagefiles = self.scan_for_categorised_files(self.context.IMAGES_DIR, categoryset)
+        # imagefiles = self.scan_for_categorised_files(self.context.IMAGES_DIR, categoryset)
         orphanassemblyfiles = set(assemblyfiles) - allincludedfileset
-        orphanmodulefiles   = set(modulefiles) - allincludedfileset
+        orphanmodulefiles = set(modulefiles) - allincludedfileset
         # Report
         for orphanassemblyfile in orphanassemblyfiles:
-            print orphanassemblyfile
+            print
+            orphanassemblyfile
         for orphanmodulefile in orphanmodulefiles:
-            print orphanmodulefile
-
+            print
+            orphanmodulefile
 
     def mv(self, args):
         frompattern = os.path.normpath(args.FROM_FILE)
@@ -1400,10 +1445,12 @@
             parentassemblies, assemblyincludes = self._scan_for_parent_assemblies(assemblyfiles + bookfiles)
             self._mv_single_file(parentassemblies, fromfile=frompattern, tofile=topattern)
         elif frompattern.count('{}') != 1:
-            print 'ERROR: More than one glob pattern {} is not allowed in FROM_FILE'
+            print
+            'ERROR: More than one glob pattern {} is not allowed in FROM_FILE'
             sys.exit()
         elif topattern.count('{}') != 1:
-            print 'ERROR: TO_FILE must contain a {} substitution pattern'
+            print
+            'ERROR: TO_FILE must contain a {} substitution pattern'
             sys.exit()
         else:
             fromprefix, fromsuffix = frompattern.split('{}')
@@ -1411,10 +1458,10 @@
             fromsuffixlen = len(fromsuffix)
             fromfiles = glob.glob(frompattern.replace('{}', '*'))
             for fromfile in fromfiles:
-                if fromsuffixlen==0:
-                    fromfilling = fromfile[fromprefixlen :]
+                if fromsuffixlen == 0:
+                    fromfilling = fromfile[fromprefixlen:]
                 else:
-                    fromfilling = fromfile[fromprefixlen : -fromsuffixlen]
+                    fromfilling = fromfile[fromprefixlen: -fromsuffixlen]
                 toprefix, tosuffix = topattern.split('{}')
                 tofile = toprefix + fromfilling + tosuffix
                 # Generate a database of parent assemblies
@@ -1424,18 +1471,19 @@
                 parentassemblies, assemblyincludes = self._scan_for_parent_assemblies(assemblyfiles + bookfiles)
                 self._mv_single_file(parentassemblies, fromfile, tofile)
 
-
     def _mv_single_file(self, parentassemblies, fromfile, tofile):
         # Perform basic sanity checks
         if not os.path.exists(fromfile):
-            print 'WARN: Origin file does not exist (skipping): ' + fromfile
+            print
+            'WARN: Origin file does not exist (skipping): ' + fromfile
             return
         if os.path.exists(tofile):
-            print 'WARN: File already exists at destination (skipping)' + tofile
+            print
+            'WARN: File already exists at destination (skipping)' + tofile
             return
         # Make sure that the destination directory exists
         destination_dir, basename = os.path.split(tofile)
-        if destination_dir!='' and not os.path.exists(destination_dir):
+        if destination_dir != '' and not os.path.exists(destination_dir):
             os.makedirs(destination_dir)
         # Move the file
         os.rename(fromfile, tofile)
@@ -1444,7 +1492,6 @@
             for parentassembly in parentassemblies[fromfile]:
                 self._rename_included_file(parentassembly, fromfile, tofile)
 
-
     def _rename_included_file(self, file, fromfile, tofile):
         # Ignore include paths with attribute substitutions
         regexp = re.compile(r'^\s*include::([^\[\{]+)\[([^\]]*)\]')
@@ -1459,7 +1506,8 @@
                         if result is not None:
                             includepath = result.group(1)
                             # Compute unique relative path, factoring out any symbolic links
-                            testpath = os.path.relpath(os.path.realpath(os.path.normpath(os.path.join(dirname, includepath))))
+                            testpath = os.path.relpath(
+                                os.path.realpath(os.path.normpath(os.path.join(dirname, includepath))))
                             if testpath == os.path.normpath(fromfile):
                                 if basename == 'master.adoc':
                                     newincludepath = tofile
@@ -1473,7 +1521,6 @@
         # Move new file
         shutil.move(abs_path, file)
 
-
     def _add_contexts(self, assemblyfiles, modulefiles, attrfilelist, args):
         # Set of files to which contexts should be added
         fixfileset = set(assemblyfiles) | set(modulefiles)
@@ -1481,7 +1528,8 @@
         if attrfilelist is not None:
             self.context.parse_attribute_files(attrfilelist)
         else:
-            print 'WARNING: No attribute files specified'
+            print
+            'WARNING: No attribute files specified'
 
         # Define some enums for state machine
         REGULAR_LINES = 0
@@ -1496,7 +1544,8 @@
         regexp_title = re.compile(r'^(=+)\s+(\S.*)')
 
         for fixfile in fixfileset:
-            print 'Adding contexts to file: ' + fixfile
+            print
+            'Adding contexts to file: ' + fixfile
             # Initialize Boolean state variables
             parsing_state = REGULAR_LINES
             # Initialize loop variables
@@ -1559,7 +1608,8 @@
                                     else:
                                         ctx_segment = title_id
                                 else:
-                                    print 'ERROR: Expected ID definition before heading = ' + title
+                                    print
+                                    'ERROR: Expected ID definition before heading = ' + title
                                     sys.exit()
                                 new_file.write(line)
                                 continue
@@ -1572,7 +1622,8 @@
                                     new_file.write(':context: {parent-of-context-' + title_id_sha + '}\n')
                                     continue
                                 else:
-                                    print 'ERROR: Expected assembly title before first include'
+                                    print
+                                    'ERROR: Expected assembly title before first include'
                                     sys.exit()
                             # Process :parent-of-context-<SHA>: {context} line
                             if is_assembly and line.startswith(':parent-of-context-'):
@@ -1581,7 +1632,8 @@
                                     new_file.write(':parent-of-context-' + title_id_sha + ': {context}\n')
                                     continue
                                 else:
-                                    print 'ERROR: Expected assembly title before first instance of :parent-of-context-<SHA>:'
+                                    print
+                                    'ERROR: Expected assembly title before first instance of :parent-of-context-<SHA>:'
                                     sys.exit()
                             # Process regular line
                             new_file.write(line)
@@ -1593,7 +1645,8 @@
                                 new_file.write(':context: {context}-' + ctx_segment + '\n')
                                 continue
                             else:
-                                print 'ERROR: Expected context definition'
+                                print
+                                'ERROR: Expected context definition'
                                 sys.exit()
                         elif parsing_state == EXPECTING_INCLUDE:
                             # Process include:: line
@@ -1602,7 +1655,8 @@
                                 new_file.write(line)
                                 continue
                             else:
-                                print 'ERROR: Expected include line'
+                                print
+                                'ERROR: Expected include line'
                                 sys.exit()
                         elif parsing_state == EXPECTING_CONTEXT_RESTORE:
                             # Process :context: {parent-of-context-<SHA>} line
@@ -1611,34 +1665,34 @@
                                 new_file.write(':context: {parent-of-context-' + title_id_sha + '}\n')
                                 continue
                             else:
-                                print 'ERROR: Expected context restore line'
+                                print
+                                'ERROR: Expected context restore line'
                                 sys.exit()
             if parsing_state == LEGACY_MODE:
                 # Leave legacy files unchanged!
-                print '  - legacy file detected - no changes made'
+                print
+                '  - legacy file detected - no changes made'
             else:
                 # Remove original file
                 os.remove(fixfile)
                 # Move new file
                 shutil.move(abs_path, fixfile)
 
-
     def _generate_hash(self, text):
         # Generates a 6-character hex encoded hash
         hash = hashlib.sha256(text.encode('UTF-8')).hexdigest()
         truncated_hash = hash[:6]
         return truncated_hash
 
-
     def toc(self, args):
         pass
 
-
     def atom(self, args):
         head, tail = os.path.split(args.FILE)
         type = self.type_of_file(tail)
         if type not in ['assembly', 'procedure', 'concept', 'reference']:
-            print 'ERROR: File must be a module or an assembly: ' + args.FILE
+            print
+            'ERROR: File must be a module or an assembly: ' + args.FILE
             sys.exit()
         if type in ['procedure', 'concept', 'reference']:
             type = 'module'
@@ -1648,7 +1702,7 @@
                 edit_parent = False
                 edit_siblings = False
                 edit_children = True
-            else: # type == 'module'
+            else:  # type == 'module'
                 edit_parent = True
                 edit_siblings = True
                 edit_children = False
@@ -1670,7 +1724,8 @@
                     if edit_siblings and (parentassembly in assemblyincludes):
                         targetfilelist.extend(assemblyincludes[parentassembly])
             else:
-                print 'WARN: Could not find parent assembly'
+                print
+                'WARN: Could not find parent assembly'
         if not edit_siblings:
             targetfilelist.append(args.FILE)
         if edit_children and type == 'assembly':
@@ -1678,18 +1733,19 @@
                 targetfilelist.extend(assemblyincludes[args.FILE])
         subprocess.check_call(['atom'] + targetfilelist)
 
-
     def version(self, args):
         pass
 
 
 def add_module_arguments(parser):
-    parser.add_argument('CATEGORY', help='Category in which to store this module. Can use / as a separator to define sub-categories')
+    parser.add_argument('CATEGORY',
+                        help='Category in which to store this module. Can use / as a separator to define sub-categories')
     parser.add_argument('MODULE_ID', help='Unique ID to identify this module')
     parser.add_argument('-u', '--user-story', help='Text of a user story (enclose in quotes)')
     parser.add_argument('-t', '--title', help='Title of the module (enclose in quotes)')
     parser.add_argument('-j', '--jira', help='Reference to a Jira issue related to the creation of this module')
-    parser.add_argument('-p', '--parent-assemblies', help='List of assemblies that include this module, specified as a space-separated list (enclose in quotes)')
+    parser.add_argument('-p', '--parent-assemblies',
+                        help='List of assemblies that include this module, specified as a space-separated list (enclose in quotes)')
 
 
 # MAIN CODE - PROGRAM STARTS HERE!
@@ -1697,8 +1753,9 @@
 
 # Basic initialization
 if not os.path.exists('nebel.cfg'):
-  print 'WARN: No nebel.cfg file found in this directory.'
-  sys.exit()
+    print
+    'WARN: No nebel.cfg file found in this directory.'
+    sys.exit()
 context = nebel.context.NebelContext()
 context.initializeFromFile('nebel.cfg')
 this_script_path = os.path.dirname(os.path.abspath(__file__))
@@ -1732,29 +1789,40 @@
 reference_parser.set_defaults(func=tasks.create_reference)
 
 # Create the sub-parser for the 'create-from' command
-create_parser = subparsers.add_parser('create-from', help='Create multiple assemblies/modules from a CSV file, or an assembly file')
-create_parser.add_argument('FROM_FILE', help='Can be either a comma-separated values (CSV) file (ending with .csv), or an assembly file (starting with {}/ and ending with .adoc)'.format(context.ASSEMBLIES_DIR))
+create_parser = subparsers.add_parser('create-from',
+                                      help='Create multiple assemblies/modules from a CSV file, or an assembly file')
+create_parser.add_argument('FROM_FILE',
+                           help='Can be either a comma-separated values (CSV) file (ending with .csv), or an assembly file (starting with {}/ and ending with .adoc)'.format(
+                               context.ASSEMBLIES_DIR))
 create_parser.set_defaults(func=tasks.create_from)
 
 # Create the sub-parser for the 'split' command
-split_parser = subparsers.add_parser('split', help='Split an annotated AsciiDoc file into multiple assemblies and modules')
-split_parser.add_argument('FROM_FILE', help='Annotated AsciiDoc file (ending with .adoc, including optional wildcard braces, {})')
-split_parser.add_argument('--legacybasedir', help='Base directory for annotated file content. Subdirectories of this directory are used as default categories.')
-split_parser.add_argument('--category-prefix', help='When splitting an annotated file, add this prefix to default categories.')
+split_parser = subparsers.add_parser('split',
+                                     help='Split an annotated AsciiDoc file into multiple assemblies and modules')
+split_parser.add_argument('FROM_FILE',
+                          help='Annotated AsciiDoc file (ending with .adoc, including optional wildcard braces, {})')
+split_parser.add_argument('--legacybasedir',
+                          help='Base directory for annotated file content. Subdirectories of this directory are used as default categories.')
+split_parser.add_argument('--category-prefix',
+                          help='When splitting an annotated file, add this prefix to default categories.')
 split_parser.add_argument('-a', '--attribute-files', help='Specify a comma-separated list of attribute files')
-split_parser.add_argument('--conditions', help='Define a comma-separated list of condition attributes, for resolving ifdef and ifndef directives')
-split_parser.add_argument('--timestamp', help='Generate a timestamp in the generated module and assembly files', action='store_true')
+split_parser.add_argument('--conditions',
+                          help='Define a comma-separated list of condition attributes, for resolving ifdef and ifndef directives')
+split_parser.add_argument('--timestamp', help='Generate a timestamp in the generated module and assembly files',
+                          action='store_true')
 split_parser.set_defaults(func=tasks.adoc_split)
 
 # Create the sub-parser for the 'book' command
 book_parser = subparsers.add_parser('book', help='Create and manage book directories')
 book_parser.add_argument('BOOK_DIR', help='The book directory')
 book_parser.add_argument('--create', help='Create a new book directory', action='store_true')
-book_parser.add_argument('-c', '--category-list', help='Comma-separated list of categories to add to book (enclose in quotes)')
+book_parser.add_argument('-c', '--category-list',
+                         help='Comma-separated list of categories to add to book (enclose in quotes)')
 book_parser.set_defaults(func=tasks.book)
 
 # Create the sub-parser for the 'mv' command
-book_parser = subparsers.add_parser('mv', help='Move (or rename) module or assembly files. You can optionally use a single instance of braces for globbing/substituting. For example, to change a file prefix from p_ to proc_ you could enter: nebel mv p_{}.adoc proc_{}.adoc')
+book_parser = subparsers.add_parser('mv',
+                                    help='Move (or rename) module or assembly files. You can optionally use a single instance of braces for globbing/substituting. For example, to change a file prefix from p_ to proc_ you could enter: nebel mv p_{}.adoc proc_{}.adoc')
 book_parser.add_argument('FROM_FILE', help='File origin. Optionally use {} for globbing.')
 book_parser.add_argument('TO_FILE', help='File destination. Optionally use {} to substitute captured glob content')
 book_parser.set_defaults(func=tasks.mv)
@@ -1763,36 +1831,46 @@
 update_parser = subparsers.add_parser('update', help='Update metadata in modules and assemblies')
 update_parser.add_argument('--fix-includes', help='Fix erroneous include directives in assemblies', action='store_true')
 update_parser.add_argument('--fix-links', help='Fix erroneous cross-reference links', action='store_true')
-update_parser.add_argument('-p','--parent-assemblies', help='Update ParentAssemblies property in modules and assemblies', action='store_true')
+update_parser.add_argument('-p', '--parent-assemblies',
+                           help='Update ParentAssemblies property in modules and assemblies', action='store_true')
 update_parser.add_argument('--generate-ids', help='Generate missing IDs for headings', action='store_true')
-update_parser.add_argument('--add-contexts', help='Add _{context} to IDs and add boilerplate around include directives', action='store_true')
-update_parser.add_argument('--hash-contexts', help='Use together with --add-contexts if you want contexts to contain hashes instead of literal IDs', action='store_true')
-update_parser.add_argument('-c', '--category-list', help='Apply update only to this comma-separated list of categories (enclose in quotes)')
+update_parser.add_argument('--add-contexts', help='Add _{context} to IDs and add boilerplate around include directives',
+                           action='store_true')
+update_parser.add_argument('--hash-contexts',
+                           help='Use together with --add-contexts if you want contexts to contain hashes instead of literal IDs',
+                           action='store_true')
+update_parser.add_argument('-c', '--category-list',
+                           help='Apply update only to this comma-separated list of categories (enclose in quotes)')
 update_parser.add_argument('-b', '--book', help='Apply update only to the specified book')
 update_parser.add_argument('-a', '--attribute-files', help='Specify a comma-separated list of attribute files')
-update_parser.add_argument('FILE', help='File to update OR you can omit this argument and use --book or --category-list instead', nargs='?')
+update_parser.add_argument('FILE',
+                           help='File to update OR you can omit this argument and use --book or --category-list instead',
+                           nargs='?')
 update_parser.set_defaults(func=tasks.update)
 
 # Create the sub-parser for the 'orphan' command
 orphan_parser = subparsers.add_parser('orphan', help='Search for orphaned module and assembly files')
-orphan_parser.add_argument('-c', '--category-list', help='Filter for orphan files belonging to this comma-separated list of categories')
+orphan_parser.add_argument('-c', '--category-list',
+                           help='Filter for orphan files belonging to this comma-separated list of categories')
 orphan_parser.add_argument('-a', '--attribute-files', help='Specify a comma-separated list of attribute files')
 orphan_parser.set_defaults(func=tasks.orphan_search)
 
 # Create the sub-parser for the 'toc' command
 toc_parser = subparsers.add_parser('toc', help='List TOC for assembly or book')
-toc_parser.add_argument('ASSEMBLY_OR_BOOK_FILE', help='Path to the assembly or book file whose table of contents you want to list')
+toc_parser.add_argument('ASSEMBLY_OR_BOOK_FILE',
+                        help='Path to the assembly or book file whose table of contents you want to list')
 toc_parser.set_defaults(func=tasks.toc)
 
 # Create the sub-parser for the 'atom' command
 atom_parser = subparsers.add_parser('atom', help='Open a module or an assembly using the atom editor')
 atom_parser.add_argument('FILE', help='Pathname of the assembly or module file to edit')
-atom_parser.add_argument('-p', '--parent', help='Open the parent assembly of the specified assembly or module', action='store_true')
-atom_parser.add_argument('-s', '--siblings', help='Open the siblings of the specified assembly or module', action='store_true')
+atom_parser.add_argument('-p', '--parent', help='Open the parent assembly of the specified assembly or module',
+                         action='store_true')
+atom_parser.add_argument('-s', '--siblings', help='Open the siblings of the specified assembly or module',
+                         action='store_true')
 atom_parser.add_argument('-c', '--children', help='Open the children of the specified assembly', action='store_true')
 atom_parser.set_defaults(func=tasks.atom)
 
-
 # Now, parse the args and call the relevant sub-command
 args = parser.parse_args()
 args.func(args)
Index: bin/nebel
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/bin/bash\nSCRIPTDIR=$(dirname $0)\nexport PYTHONPATH=$SCRIPTDIR/..\n\n#python -m cProfile -s ncalls $PYTHONPATH/nebel/commands.py \"$@\"\npython2 $PYTHONPATH/nebel/commands.py \"$@\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/bin/nebel b/bin/nebel
--- a/bin/nebel	(revision 1bc0b02d66cdf9081a5604adef47921f6e7eb456)
+++ b/bin/nebel	(date 1654624890640)
@@ -3,4 +3,4 @@
 export PYTHONPATH=$SCRIPTDIR/..
 
 #python -m cProfile -s ncalls $PYTHONPATH/nebel/commands.py "$@"
-python2 $PYTHONPATH/nebel/commands.py "$@"
+python3 $PYTHONPATH/nebel/commands.py "$@"
Index: nebel/factory.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>'''\nCreated on January 2, 2019\n\n@author fbolton\n'''\n\nimport os\nimport sys\nimport re\n\nclass ModuleFactory:\n    def __init__(self, context):\n        self.context = context\n\n    def lreplace(self, pat, sub, target):\n        if target.startswith(pat):\n            return sub + target[len(pat):]\n        else:\n            return target\n\n    def name_of_file(self, metadata):\n        type = metadata['Type'].lower()\n        moduleid = metadata['ModuleID']\n        if not moduleid.endswith('{context}'):\n            coremoduleid = moduleid\n        else:\n            tmpstr = moduleid.replace('{context}', '')\n            regexp = re.compile(r'[_\\-]+$')\n            result = regexp.search(tmpstr)\n            if result is None:\n                print 'ERROR: Cannot parse ModuleID: ' + moduleid\n                sys.exit()\n            coremoduleid = regexp.sub('', tmpstr)\n        coremoduleid = coremoduleid.replace('_', '-')\n        if type == 'assembly' and not coremoduleid.startswith(self.context.ASSEMBLY_PREFIX):\n            return self.context.ASSEMBLY_PREFIX + coremoduleid + '.adoc'\n        elif type == 'procedure' and not coremoduleid.startswith(self.context.PROCEDURE_PREFIX):\n            return self.context.PROCEDURE_PREFIX + coremoduleid + '.adoc'\n        elif type == 'concept' and not coremoduleid.startswith(self.context.CONCEPT_PREFIX):\n            return self.context.CONCEPT_PREFIX + coremoduleid + '.adoc'\n        elif type == 'reference' and not coremoduleid.startswith(self.context.REFERENCE_PREFIX):\n            return self.context.REFERENCE_PREFIX + coremoduleid + '.adoc'\n        else:\n            # For a generic module of unknown type, do not attach a prefix\n            return coremoduleid + '.adoc'\n\n    def normalize_filename(self, filename):\n        normalized = filename.replace('_', '-')\n        return normalized\n\n\n    def module_dirpath(self, metadata):\n        category = metadata['Category']\n        type = metadata['Type'].lower()\n        if type == 'assembly':\n            return os.path.join(self.context.ASSEMBLIES_DIR, category)\n        elif type in ['procedure', 'concept', 'reference', 'module']:\n            return os.path.join(self.context.MODULES_DIR, category)\n        else:\n            print 'ERROR: Unknown module Type: ' + str(type)\n            sys.exit()\n\n    def module_or_assembly_path(self, metadata):\n        return os.path.join(self.module_dirpath(metadata), self.name_of_file(metadata))\n\n    def create(self, metadata, filecontents = None, clobber = False):\n        type = metadata['Type'].lower()\n        filename = self.name_of_file(metadata)\n        dirpath = self.module_dirpath(metadata)\n        if not os.path.exists(dirpath):\n            os.makedirs(dirpath)\n        filepath = os.path.join(dirpath, filename)\n        if os.path.exists(filepath) and not clobber:\n            print 'INFO: File already exists, skipping: ' + filename\n            return filepath\n        with open(filepath, 'w') as filehandle:\n            filehandle.write('// Metadata created by nebel\\n')\n            filehandle.write('//\\n')\n            for field in self.context.optionalMetadataFields:\n                if (field in metadata) and (field.lower() != 'title') and (field.lower() != 'includefiles'):\n                    filehandle.write('// ' + field + ': ' + metadata[field] + '\\n')\n            filehandle.write('\\n')\n            filehandle.write('[id=\"' + metadata['ModuleID'] + '\"]\\n')\n            if filecontents is not None:\n                # If filecontents is provided, write the contents verbatim\n                filehandle.write('= ' + metadata['Title'] + '\\n')\n                filehandle.writelines(filecontents)\n                return filepath\n            elif type == 'module':\n                # Cannot use a template, because we do not know the exact module type\n                filehandle.write('= ' + metadata['Title'] + '\\n')\n                return filepath\n            else:\n                # Generate contents from template\n                templatefile = os.path.join(self.context.templatePath, type + '.adoc')\n                with open(templatefile, 'r') as templatehandle:\n                    if 'Title' in metadata:\n                        # Replace the title from the first line of the template\n                        templatehandle.readline()\n                        filehandle.write('= ' + metadata['Title'] + '\\n')\n                    # Process the rest of the file\n                    for line in templatehandle:\n                        if line.startswith('//INCLUDE') and ('IncludeFiles' in metadata):\n                            for includedfilepath in metadata['IncludeFiles'].split(','):\n                                filehandle.write('include::' + os.path.relpath(includedfilepath, dirpath) + '[leveloffset=+1]\\n\\n')\n                        else:\n                            filehandle.write(line)\n        return filepath\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/nebel/factory.py b/nebel/factory.py
--- a/nebel/factory.py	(revision 1bc0b02d66cdf9081a5604adef47921f6e7eb456)
+++ b/nebel/factory.py	(date 1654624890629)
@@ -5,8 +5,9 @@
 '''
 
 import os
-import sys
 import re
+import sys
+
 
 class ModuleFactory:
     def __init__(self, context):
@@ -28,7 +29,8 @@
             regexp = re.compile(r'[_\-]+$')
             result = regexp.search(tmpstr)
             if result is None:
-                print 'ERROR: Cannot parse ModuleID: ' + moduleid
+                print
+                'ERROR: Cannot parse ModuleID: ' + moduleid
                 sys.exit()
             coremoduleid = regexp.sub('', tmpstr)
         coremoduleid = coremoduleid.replace('_', '-')
@@ -48,7 +50,6 @@
         normalized = filename.replace('_', '-')
         return normalized
 
-
     def module_dirpath(self, metadata):
         category = metadata['Category']
         type = metadata['Type'].lower()
@@ -57,13 +58,14 @@
         elif type in ['procedure', 'concept', 'reference', 'module']:
             return os.path.join(self.context.MODULES_DIR, category)
         else:
-            print 'ERROR: Unknown module Type: ' + str(type)
+            print
+            'ERROR: Unknown module Type: ' + str(type)
             sys.exit()
 
     def module_or_assembly_path(self, metadata):
         return os.path.join(self.module_dirpath(metadata), self.name_of_file(metadata))
 
-    def create(self, metadata, filecontents = None, clobber = False):
+    def create(self, metadata, filecontents=None, clobber=False):
         type = metadata['Type'].lower()
         filename = self.name_of_file(metadata)
         dirpath = self.module_dirpath(metadata)
@@ -71,7 +73,8 @@
             os.makedirs(dirpath)
         filepath = os.path.join(dirpath, filename)
         if os.path.exists(filepath) and not clobber:
-            print 'INFO: File already exists, skipping: ' + filename
+            print
+            'INFO: File already exists, skipping: ' + filename
             return filepath
         with open(filepath, 'w') as filehandle:
             filehandle.write('// Metadata created by nebel\n')
@@ -102,7 +105,8 @@
                     for line in templatehandle:
                         if line.startswith('//INCLUDE') and ('IncludeFiles' in metadata):
                             for includedfilepath in metadata['IncludeFiles'].split(','):
-                                filehandle.write('include::' + os.path.relpath(includedfilepath, dirpath) + '[leveloffset=+1]\n\n')
+                                filehandle.write(
+                                    'include::' + os.path.relpath(includedfilepath, dirpath) + '[leveloffset=+1]\n\n')
                         else:
                             filehandle.write(line)
         return filepath
Index: nebel/context.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>'''\nCreated on January 2, 2019\n\n@author fbolton\n'''\n\nimport re\nimport sys\nimport ConfigParser\n\n\nclass NebelContext:\n    def __init__(self):\n        self.mandatoryMetadataFields = {\n            'Type',\n            'Category',\n            'ModuleID'\n        }\n        self.optionalMetadataFields = {\n            'ParentAssemblies',\n            'UserStory',\n            'VerifiedInVersion',\n            'QuickstartID',\n            'Jira',\n            'Title',\n            'IncludeFiles',\n            'ConversionStatus',\n            'ConversionDate',\n            'ConvertedFromFile',\n            'ConvertedFromID',\n            'ConvertedFromTitle',\n            'Level',\n            'TargetModQuality'\n        }\n        self.allMetadataFields = self.mandatoryMetadataFields | self.optionalMetadataFields\n        self.templatePath = ''\n        self.moduleFactory = None\n        self.attributeDict = {}\n        self.bookUrlAttributes = {}\n        self.ASSEMBLIES_DIR = 'assemblies'\n        self.MODULES_DIR = 'modules'\n        self.IMAGES_DIR = 'images'\n        self.ASSEMBLY_PREFIX = 'assembly-'\n        self.PROCEDURE_PREFIX = 'proc-'\n        self.CONCEPT_PREFIX = 'con-'\n        self.REFERENCE_PREFIX = 'ref-'\n\n    def initializeFromFile(self, configfile):\n        # print 'Initializing from file: ' + configfile\n        config = ConfigParser.RawConfigParser(\n            {'dir.assemblies': self.ASSEMBLIES_DIR,\n             'dir.modules': self.MODULES_DIR,\n             'dir.images': self.IMAGES_DIR,\n             'prefix.assembly': self.ASSEMBLY_PREFIX,\n             'prefix.procedure': self.PROCEDURE_PREFIX,\n             'prefix.concept': self.CONCEPT_PREFIX,\n             'prefix.reference': self.REFERENCE_PREFIX}\n        )\n        config.read(configfile)\n        if config.has_section('Nebel'):\n            self.ASSEMBLIES_DIR   = config.get('Nebel', 'dir.assemblies')\n            self.MODULES_DIR      = config.get('Nebel', 'dir.modules')\n            self.IMAGES_DIR       = config.get('Nebel', 'dir.images')\n            self.ASSEMBLY_PREFIX  = config.get('Nebel', 'prefix.assembly')\n            self.PROCEDURE_PREFIX = config.get('Nebel', 'prefix.procedure')\n            self.CONCEPT_PREFIX   = config.get('Nebel', 'prefix.concept')\n            self.REFERENCE_PREFIX = config.get('Nebel', 'prefix.reference')\n\n    def parse_attribute_files(self, filelist):\n        regexp = re.compile(r'^:([\\w\\-]+):\\s+(.*)')\n        for file in filelist:\n            with open(file, 'r') as f:\n                for line in f:\n                    result = regexp.search(line)\n                    if result is not None:\n                        name = result.group(1)\n                        value = result.group(2).strip()\n                        self.attributeDict[name] = [value, None]\n        for name in self.attributeDict:\n            self.attributeDict[name][1] = self.resolve_raw_attribute_value(self.attributeDict[name][0])\n        #for (name,duple) in self.attributeDict.items():\n        #    print name + ': ' + duple[0] + ', ' + duple[1]\n        self.scan_attributes_for_book_urls()\n        # print self.bookUrlAttributes\n\n\n    def update_attribute(self, name, value):\n        # Adds a new attribute to the dictionary OR updates an existing entry\n        if value is not None and value != '':\n            resolved_value = self.resolve_raw_attribute_value(value)\n        else:\n            resolved_value = value\n        self.attributeDict[name] = [value, resolved_value]\n\n\n    def lookup_attribute(self, name):\n        if name in self.attributeDict:\n            return self.attributeDict[name][1]\n        else:\n            return None\n\n\n    def clear_attributes(self):\n        self.attributeDict.clear()\n\n\n    def resolve_raw_attribute_value(self, value):\n        if len(self.attributeDict) == 0:\n            return value\n        regexp = re.compile(r'\\{([\\w\\-]+)\\}')\n        new_value = regexp.sub(self.replace_matching_attribute, value)\n        return new_value\n\n\n    def replace_matching_attribute(self, match_obj):\n        name = match_obj.group(1)\n        if not self.attributeDict.has_key(name):\n            print 'WARNING: Attribute {' + name + '} cannot be resolved.'\n            # Treat it as a literal value in braces\n            value = '{' + name + '}'\n            self.attributeDict[name] = [value, value]\n        duple = self.attributeDict[name]\n        if duple[1] is None:\n            duple[1] = self.resolve_raw_attribute_value(duple[0])\n        return duple[1]\n\n\n    def scan_attributes_for_book_urls(self):\n        regexp = re.compile(r'https://access.redhat.com/documentation/en-us/([^/]+)/([^/]+)/html-single/([^/]+)/?')\n        for name in self.attributeDict:\n            resolved_value = self.attributeDict[name][1]\n            result = regexp.search(resolved_value)\n            if result is not None:\n                productpkg = result.group(1)\n                version = result.group(2)\n                bookslug = result.group(3)\n                if productpkg not in self.bookUrlAttributes:\n                    self.bookUrlAttributes[productpkg] = {}\n                self.bookUrlAttributes[productpkg][bookslug] = name\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/nebel/context.py b/nebel/context.py
--- a/nebel/context.py	(revision 1bc0b02d66cdf9081a5604adef47921f6e7eb456)
+++ b/nebel/context.py	(date 1654624890652)
@@ -5,7 +5,7 @@
 '''
 
 import re
-import sys
+
 import ConfigParser
 
 
@@ -58,12 +58,12 @@
         )
         config.read(configfile)
         if config.has_section('Nebel'):
-            self.ASSEMBLIES_DIR   = config.get('Nebel', 'dir.assemblies')
-            self.MODULES_DIR      = config.get('Nebel', 'dir.modules')
-            self.IMAGES_DIR       = config.get('Nebel', 'dir.images')
-            self.ASSEMBLY_PREFIX  = config.get('Nebel', 'prefix.assembly')
+            self.ASSEMBLIES_DIR = config.get('Nebel', 'dir.assemblies')
+            self.MODULES_DIR = config.get('Nebel', 'dir.modules')
+            self.IMAGES_DIR = config.get('Nebel', 'dir.images')
+            self.ASSEMBLY_PREFIX = config.get('Nebel', 'prefix.assembly')
             self.PROCEDURE_PREFIX = config.get('Nebel', 'prefix.procedure')
-            self.CONCEPT_PREFIX   = config.get('Nebel', 'prefix.concept')
+            self.CONCEPT_PREFIX = config.get('Nebel', 'prefix.concept')
             self.REFERENCE_PREFIX = config.get('Nebel', 'prefix.reference')
 
     def parse_attribute_files(self, filelist):
@@ -78,12 +78,11 @@
                         self.attributeDict[name] = [value, None]
         for name in self.attributeDict:
             self.attributeDict[name][1] = self.resolve_raw_attribute_value(self.attributeDict[name][0])
-        #for (name,duple) in self.attributeDict.items():
+        # for (name,duple) in self.attributeDict.items():
         #    print name + ': ' + duple[0] + ', ' + duple[1]
         self.scan_attributes_for_book_urls()
         # print self.bookUrlAttributes
 
-
     def update_attribute(self, name, value):
         # Adds a new attribute to the dictionary OR updates an existing entry
         if value is not None and value != '':
@@ -92,18 +91,15 @@
             resolved_value = value
         self.attributeDict[name] = [value, resolved_value]
 
-
     def lookup_attribute(self, name):
         if name in self.attributeDict:
             return self.attributeDict[name][1]
         else:
             return None
 
-
     def clear_attributes(self):
         self.attributeDict.clear()
 
-
     def resolve_raw_attribute_value(self, value):
         if len(self.attributeDict) == 0:
             return value
@@ -111,11 +107,11 @@
         new_value = regexp.sub(self.replace_matching_attribute, value)
         return new_value
 
-
     def replace_matching_attribute(self, match_obj):
         name = match_obj.group(1)
         if not self.attributeDict.has_key(name):
-            print 'WARNING: Attribute {' + name + '} cannot be resolved.'
+            print
+            'WARNING: Attribute {' + name + '} cannot be resolved.'
             # Treat it as a literal value in braces
             value = '{' + name + '}'
             self.attributeDict[name] = [value, value]
@@ -124,7 +120,6 @@
             duple[1] = self.resolve_raw_attribute_value(duple[0])
         return duple[1]
 
-
     def scan_attributes_for_book_urls(self):
         regexp = re.compile(r'https://access.redhat.com/documentation/en-us/([^/]+)/([^/]+)/html-single/([^/]+)/?')
         for name in self.attributeDict:
Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>.idea/\n*.pyc\n*.html\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.gitignore b/.gitignore
--- a/.gitignore	(revision 1bc0b02d66cdf9081a5604adef47921f6e7eb456)
+++ b/.gitignore	(date 1537510512000)
@@ -1,3 +1,99 @@
-.idea/
-*.pyc
-*.html
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+
+# C extensions
+*.so
+
+# Distribution / packaging
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+.hypothesis/
+
+# Translations
+*.mo
+*.pot
+
+# Django stuff:
+*.log
+local_settings.py
+
+# Flask stuff:
+instance/
+.webassets-cache
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/_build/
+
+# PyBuilder
+target/
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# pyenv
+.python-version
+
+# celery beat schedule file
+celerybeat-schedule
+
+# SageMath parsed files
+*.sage.py
+
+# Environments
+.env
+.venv
+env/
+venv/
+ENV/
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# Rope project settings
+.ropeproject
+
+# mkdocs documentation
+/site
+
+# mypy
+.mypy_cache/
Index: test/test_sample.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/test_sample.py b/test/test_sample.py
new file mode 100644
--- /dev/null	(date 1537510512000)
+++ b/test/test_sample.py	(date 1537510512000)
@@ -0,0 +1,15 @@
+"""
+You can auto-discover and run all tests with this command:
+
+    py.test
+
+Documentation: https://docs.pytest.org/en/latest/
+"""
+
+
+def inc(x):
+    return x + 1
+
+
+def test_answer():
+    assert inc(3) == 4
Index: VERSIONS.adoc
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/VERSIONS.adoc b/VERSIONS.adoc
new file mode 100644
--- /dev/null	(date 1654626097089)
+++ b/VERSIONS.adoc	(date 1654626097089)
@@ -0,0 +1,6 @@
+= Nebel Version history:
+
+* 2.1.x -- Supports the new `nebel split` subcommand.
+* 2.0.x -- Backwards-incompatible update, uses the new convention for modular file prefixes (separator character is part of the customizable prefix, thus enabling you to use a hyphen separator).
+* 1.0.0 -- First numbered version (from April 4, 2020), uses the old convention for modular file prefixes (underscore separator is hardcoded).
+
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
--- /dev/null	(date 1654625893927)
+++ b/requirements.txt	(date 1654625893927)
@@ -0,0 +1,3 @@
+pytest>=3.0.7
+tox>=2.7.0
+logzero  # see https://logzero.readthedocs.io/en/latest/
\ No newline at end of file
Index: nebel/main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/nebel/main.py b/nebel/main.py
new file mode 100755
--- /dev/null	(date 1654626543955)
+++ b/nebel/main.py	(date 1654626543955)
@@ -0,0 +1,48 @@
+#!/usr/bin/env python3
+"""
+Docstring TBD
+"""
+
+__author__ = "ajonsson"
+__version__ = "3.1.0"
+__license__ = "MIT"
+
+import argparse
+from logzero import logger
+
+
+def main(args):
+    """ Main entry point of the app """
+    logger.info("hello world")
+    logger.info(args)
+
+
+if __name__ == "__main__":
+    """ This is executed when run from the command line """
+    parser = argparse.ArgumentParser()
+
+    # Required positional argument
+    parser.add_argument("arg", help="Required positional argument")
+
+    # Optional argument flag which defaults to False
+    parser.add_argument("-f", "--flag", action="store_true", default=False)
+
+    # Optional argument which requires a parameter (eg. -d test)
+    parser.add_argument("-n", "--name", action="store", dest="name")
+
+    # Optional verbosity counter (eg. -v, -vv, -vvv, etc.)
+    parser.add_argument(
+        "-v",
+        "--verbose",
+        action="count",
+        default=0,
+        help="Verbosity (-v, -vv, etc)")
+
+    # Specify output of "--version"
+    parser.add_argument(
+        "--version",
+        action="version",
+        version="%(prog)s (version {version})".format(version=__version__))
+
+    args = parser.parse_args()
+    main(args)
Index: tox.ini
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tox.ini b/tox.ini
new file mode 100644
--- /dev/null	(date 1537510512000)
+++ b/tox.ini	(date 1537510512000)
@@ -0,0 +1,19 @@
+# Tox (https://tox.readthedocs.io/) is a tool for running tests
+# in multiple virtualenvs. This configuration file will run the
+# test suite on all supported python versions. To use it, "pip install tox"
+# and then run "tox" from this directory.
+#
+# See also https://tox.readthedocs.io/en/latest/config.html for more
+# configuration options.
+
+[tox]
+# Choose your Python versions. They have to be available
+# on the system the tests are run on.
+envlist = py34, py35, py36
+
+# Tell tox to not require a setup.py file
+skipsdist = True
+
+[testenv]
+deps = -rrequirements.txt
+commands = py.test
